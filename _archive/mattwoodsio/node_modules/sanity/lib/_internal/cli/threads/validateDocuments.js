'use strict';

var client = require('@sanity/client');
var types = require('@sanity/types');
var fs = require('fs');
var os = require('os');
var path = require('path');
var readline = require('readline');
var sanity = require('sanity');
var stream = require('stream');
var worker_threads = require('worker_threads');
var tar = require('tar-stream');
var zlib = require('zlib');
var getStudioWorkspaces = require('../../../_chunks/getStudioWorkspaces-BP-leiyc.js');
var mockBrowserEnvironment = require('../../../_chunks/mockBrowserEnvironment-Bpx7ZdVR.js');
var workerChannels = require('../../../_chunks/workerChannels-BZhuLtcy.js');
function _interopDefaultCompat(e) {
  return e && typeof e === 'object' && 'default' in e ? e : {
    default: e
  };
}
var fs__default = /*#__PURE__*/_interopDefaultCompat(fs);
var os__default = /*#__PURE__*/_interopDefaultCompat(os);
var path__default = /*#__PURE__*/_interopDefaultCompat(path);
var readline__default = /*#__PURE__*/_interopDefaultCompat(readline);
var tar__default = /*#__PURE__*/_interopDefaultCompat(tar);
var zlib__default = /*#__PURE__*/_interopDefaultCompat(zlib);
const HEADER_SIZE = 300;
const isGzip = buf => buf.length >= 3 && buf[0] === 31 && buf[1] === 139 && buf[2] === 8;
const isDeflate = buf => buf.length >= 2 && buf[0] === 120 && (buf[1] === 1 || buf[1] === 156 || buf[1] === 218);
const isTar = buf => buf.length >= 262 && buf[257] === 117 && buf[258] === 115 && buf[259] === 116 && buf[260] === 97 && buf[261] === 114;
async function* extract(stream, extractor) {
  const drained = new Promise((resolve, reject) => {
    setTimeout(async () => {
      try {
        for await (const chunk of stream) extractor.write(chunk);
        extractor.end();
        resolve();
      } catch (err) {
        reject(err);
      }
    });
  });
  yield* extractor;
  await drained;
  extractor.destroy();
}
async function* maybeExtractNdjson(stream) {
  let buffer = Buffer.alloc(0);
  for await (const chunk of stream) {
    buffer = Buffer.concat([buffer, chunk]);
    if (buffer.length < HEADER_SIZE) continue;
    const fileHeader = buffer;
    const restOfStream = async function* restOfStream2() {
      yield fileHeader;
      yield* stream;
    };
    if (isGzip(fileHeader)) {
      yield* maybeExtractNdjson(extract(restOfStream(), zlib__default.default.createGunzip()));
      return;
    }
    if (isDeflate(fileHeader)) {
      yield* maybeExtractNdjson(extract(restOfStream(), zlib__default.default.createDeflate()));
      return;
    }
    if (isTar(fileHeader)) {
      for await (const entry of extract(restOfStream(), tar__default.default.extract())) {
        const filename = path__default.default.basename(entry.header.name);
        const extname = path__default.default.extname(filename).toLowerCase();
        if (extname !== ".ndjson" || filename.startsWith(".")) continue;
        for await (const ndjsonChunk of entry) yield ndjsonChunk;
        return;
      }
    }
    yield* restOfStream();
  }
}
async function* extractDocumentsFromNdjsonOrTarball(file) {
  const lines = readline__default.default.createInterface({
    input: stream.Readable.from(maybeExtractNdjson(file))
  });
  for await (const line of lines) {
    const trimmed = line.trim();
    if (trimmed) yield JSON.parse(trimmed);
  }
  lines.close();
}
const MAX_VALIDATION_CONCURRENCY = 100;
const DOCUMENT_VALIDATION_TIMEOUT = 3e4;
const REFERENCE_INTEGRITY_BATCH_SIZE = 100;
const {
  clientConfig,
  workDir,
  workspace: workspaceName,
  configPath,
  dataset,
  ndjsonFilePath,
  projectId,
  level,
  maxCustomValidationConcurrency
} = worker_threads.workerData;
if (worker_threads.isMainThread || !worker_threads.parentPort) {
  throw new Error("This module must be run as a worker thread");
}
const levelValues = {
  error: 0,
  warning: 1,
  info: 2
};
const report = workerChannels.createReporter(worker_threads.parentPort);
const getReferenceIds = value => {
  const ids = /* @__PURE__ */new Set();
  function traverse(node) {
    if (types.isReference(node)) {
      ids.add(node._ref);
      return;
    }
    if (typeof node === "object" && node) {
      for (const item of Object.values(node)) traverse(item);
    }
  }
  traverse(value);
  return ids;
};
const idRegex = /^[^-][A-Z0-9._-]*$/i;
const isValidId = id => typeof id === "string" && idRegex.test(id);
const shouldIncludeDocument = document => {
  return !document._type.startsWith("system.");
};
async function* readerToGenerator(reader) {
  while (true) {
    const {
      value,
      done
    } = await reader.read();
    if (value) yield value;
    if (done) return;
  }
}
validateDocuments();
async function loadWorkspace() {
  const workspaces = await getStudioWorkspaces.getStudioWorkspaces({
    basePath: workDir,
    configPath
  });
  if (!workspaces.length) {
    throw new Error("Configuration did not return any workspaces.");
  }
  let _workspace;
  if (workspaceName) {
    _workspace = workspaces.find(w => w.name === workspaceName);
    if (!_workspace) {
      throw new Error("Could not find any workspaces with name `".concat(workspaceName, "`"));
    }
  } else {
    if (workspaces.length !== 1) {
      throw new Error("Multiple workspaces found. Please specify which workspace to use with '--workspace'.");
    }
    _workspace = workspaces[0];
  }
  const workspace = _workspace;
  const client$1 = client.createClient({
    ...clientConfig,
    dataset: dataset || workspace.dataset,
    projectId: projectId || workspace.projectId,
    requestTagPrefix: "sanity.cli.validate"
  }).config({
    apiVersion: "v2021-03-25"
  });
  let studioHost;
  try {
    const project = await client$1.projects.getById(projectId || workspace.projectId);
    studioHost = project.metadata.externalStudioHost || project.studioHost;
  } catch {
    studioHost = null;
  }
  report.event.loadedWorkspace({
    projectId: workspace.projectId,
    dataset: workspace.dataset,
    name: workspace.name,
    studioHost,
    basePath: workspace.basePath
  });
  return {
    workspace,
    client: client$1,
    studioHost
  };
}
async function downloadFromExport(client) {
  var _a;
  const exportUrl = new URL(client.getUrl("/data/export/".concat(client.config().dataset), false));
  const documentCount = await client.fetch("length(*)");
  report.event.loadedDocumentCount({
    documentCount
  });
  const {
    token
  } = client.config();
  const response = await fetch(exportUrl, {
    headers: new Headers({
      ...(token && {
        Authorization: "Bearer ".concat(token)
      })
    })
  });
  const reader = (_a = response.body) == null ? void 0 : _a.getReader();
  if (!reader) throw new Error("Could not get reader from response body.");
  let downloadedCount = 0;
  const referencedIds = /* @__PURE__ */new Set();
  const documentIds = /* @__PURE__ */new Set();
  const lines = readline__default.default.createInterface({
    input: stream.Readable.from(readerToGenerator(reader))
  });
  const slugDate = ( /* @__PURE__ */new Date()).toISOString().replace(/[^a-z0-9]/gi, "-").toLowerCase();
  const tempOutputFile = path__default.default.join(os__default.default.tmpdir(), "sanity-validate-".concat(slugDate, ".ndjson"));
  const outputStream = fs__default.default.createWriteStream(tempOutputFile);
  for await (const line of lines) {
    const document = JSON.parse(line);
    if (shouldIncludeDocument(document)) {
      documentIds.add(document._id);
      for (const referenceId of getReferenceIds(document)) {
        referencedIds.add(referenceId);
      }
      outputStream.write("".concat(line, "\n"));
    }
    downloadedCount++;
    report.stream.exportProgress.emit({
      downloadedCount,
      documentCount
    });
  }
  await new Promise((resolve, reject) => outputStream.close(err => err ? reject(err) : resolve()));
  report.stream.exportProgress.end();
  report.event.exportFinished({
    totalDocumentsToValidate: documentIds.size
  });
  const getDocuments = () => extractDocumentsFromNdjsonOrTarball(fs__default.default.createReadStream(tempOutputFile));
  return {
    documentIds,
    referencedIds,
    getDocuments,
    cleanup: () => fs__default.default.promises.rm(tempOutputFile)
  };
}
async function downloadFromFile(filePath) {
  const referencedIds = /* @__PURE__ */new Set();
  const documentIds = /* @__PURE__ */new Set();
  const getDocuments = () => extractDocumentsFromNdjsonOrTarball(fs__default.default.createReadStream(filePath));
  for await (const document of getDocuments()) {
    if (shouldIncludeDocument(document)) {
      documentIds.add(document._id);
      for (const referenceId of getReferenceIds(document)) {
        referencedIds.add(referenceId);
      }
    }
  }
  report.event.exportFinished({
    totalDocumentsToValidate: documentIds.size
  });
  return {
    documentIds,
    referencedIds,
    getDocuments,
    cleanup: void 0
  };
}
async function checkReferenceExistence(_ref) {
  let {
    client,
    documentIds,
    referencedIds: _referencedIds
  } = _ref;
  const existingIds = new Set(documentIds);
  const idsToCheck = Array.from(_referencedIds).filter(id => !existingIds.has(id) && isValidId(id)).sort();
  const batches = idsToCheck.reduce((acc, next, index) => {
    const batchIndex = Math.floor(index / REFERENCE_INTEGRITY_BATCH_SIZE);
    const batch = acc[batchIndex];
    batch.push(next);
    return acc;
  }, Array.from({
    length: Math.ceil(idsToCheck.length / REFERENCE_INTEGRITY_BATCH_SIZE)
  }).map(() => []));
  for (const batch of batches) {
    const {
      omitted
    } = await client.request({
      uri: client.getDataUrl("doc", batch.join(",")),
      json: true,
      query: {
        excludeContent: "true"
      },
      tag: "documents-availability"
    });
    const omittedIds = omitted.reduce((acc, next) => {
      acc[next.id] = next.reason;
      return acc;
    }, {});
    for (const id of batch) {
      if (omittedIds[id] !== "existence") {
        existingIds.add(id);
      }
    }
  }
  report.event.loadedReferenceIntegrity();
  return {
    existingIds
  };
}
async function validateDocuments() {
  const {
    default: pMap
  } = await import('p-map');
  const cleanupBrowserEnvironment = mockBrowserEnvironment.mockBrowserEnvironment(workDir);
  let cleanupDownloadedDocuments;
  try {
    const {
      client,
      workspace,
      studioHost
    } = await loadWorkspace();
    const {
      documentIds,
      referencedIds,
      getDocuments,
      cleanup
    } = ndjsonFilePath ? await downloadFromFile(ndjsonFilePath) : await downloadFromExport(client);
    cleanupDownloadedDocuments = cleanup;
    const {
      existingIds
    } = await checkReferenceExistence({
      client,
      referencedIds,
      documentIds
    });
    const getClient = options => client.withConfig(options);
    const getDocumentExists = _ref2 => {
      let {
        id
      } = _ref2;
      return Promise.resolve(existingIds.has(id));
    };
    const getLevel = markers => {
      let foundWarning = false;
      for (const marker of markers) {
        if (marker.level === "error") return "error";
        if (marker.level === "warning") foundWarning = true;
      }
      if (foundWarning) return "warning";
      return "info";
    };
    let validatedCount = 0;
    const validate = async document => {
      let markers;
      try {
        const timeout = Symbol("timeout");
        const result = await Promise.race([sanity.validateDocument({
          document,
          workspace,
          getClient,
          getDocumentExists,
          environment: "cli",
          maxCustomValidationConcurrency
        }), new Promise(resolve => setTimeout(() => resolve(timeout), DOCUMENT_VALIDATION_TIMEOUT))]);
        if (result === timeout) {
          throw new Error("Document '".concat(document._id, "' failed to validate within ").concat(DOCUMENT_VALIDATION_TIMEOUT, "ms."));
        }
        markers = result.map(_ref3 => {
          let {
            item,
            ...marker
          } = _ref3;
          return marker;
        }).filter(marker => {
          var _a;
          const markerValue = levelValues[marker.level];
          const flagLevelValue = (_a = levelValues[level]) != null ? _a : levelValues.info;
          return markerValue <= flagLevelValue;
        });
      } catch (err) {
        const errorMessage = sanity.isRecord(err) && typeof err.message === "string" ? err.message : "Unknown error";
        const message = "Exception occurred while validating value: ".concat(errorMessage);
        markers = [{
          message,
          level: "error",
          path: []
        }];
      }
      validatedCount++;
      const intentUrl = studioHost && "".concat(studioHost).concat(path__default.default.resolve(workspace.basePath, "/intent/edit/id=".concat(encodeURIComponent(document._id), ";type=").concat(encodeURIComponent(document._type))));
      report.stream.validation.emit({
        documentId: document._id,
        documentType: document._type,
        revision: document._rev,
        ...(intentUrl && {
          intentUrl
        }),
        markers,
        validatedCount,
        level: getLevel(markers)
      });
    };
    await pMap(getDocuments(), validate, {
      concurrency: MAX_VALIDATION_CONCURRENCY
    });
    report.stream.validation.end();
  } finally {
    await (cleanupDownloadedDocuments == null ? void 0 : cleanupDownloadedDocuments());
    cleanupBrowserEnvironment();
  }
}
//# sourceMappingURL=validateDocuments.js.map
