'use strict';

var debug$3 = require('debug');
var node_fs = require('node:fs');
var promises$1 = require('node:fs/promises');
var node_os = require('node:os');
var path = require('node:path');
var promises = require('node:stream/promises');
var fs = require('@sanity/util/fs');
var asyncMutex = require('async-mutex');
var isString = require('lodash/isString.js');
var prettyMs = require('pretty-ms');
var helpers = require('yargs/helpers');
var yargs = require('yargs/yargs');
var zlib = require('node:zlib');
var rimraf = require('rimraf');
var getIt = require('get-it');
var middleware = require('get-it/middleware');
var node_stream = require('node:stream');
var consoleTablePrinter = require('console-table-printer');
var dateFns = require('date-fns');
var fs$1 = require('fs');
var path$1 = require('path');
var logSymbols = require('log-symbols');
var oneline = require('oneline');
var url = require('url');
var EventSource = require('@sanity/eventsource');
var rxjs = require('rxjs');
var exportDataset = require('@sanity/export');
var fs$2 = require('fs/promises');
var sanityImport = require('@sanity/import');
var padStart = require('lodash/padStart');
var uuid = require('@sanity/uuid');
var chokidar = require('chokidar');
var execa = require('execa');
var json5 = require('json5');
var isEqual = require('lodash/isEqual.js');
var isPlainObject = require('lodash/isPlainObject.js');
var noop$1 = require('lodash/noop.js');
var os = require('os');
var pluralize = require('pluralize-esm');
var tokenize = require('json-lexer');
var open = require('open');
var groupBy = require('lodash/groupBy.js');
var util = require('util');
var deburr = require('lodash/deburr');
var node = require('esbuild-register/dist/node');
var migrate = require('@sanity/migrate');
var node_tty = require('node:tty');
var sanity = require('sanity');
var size = require('lodash/size.js');
var sortBy = require('lodash/sortBy.js');
function _interopDefaultCompat(e) {
  return e && typeof e === 'object' && 'default' in e ? e : {
    default: e
  };
}
var debug__default = /*#__PURE__*/_interopDefaultCompat(debug$3);
var path__default = /*#__PURE__*/_interopDefaultCompat(path);
var isString__default = /*#__PURE__*/_interopDefaultCompat(isString);
var prettyMs__default = /*#__PURE__*/_interopDefaultCompat(prettyMs);
var yargs__default = /*#__PURE__*/_interopDefaultCompat(yargs);
var zlib__default = /*#__PURE__*/_interopDefaultCompat(zlib);
var rimraf__default = /*#__PURE__*/_interopDefaultCompat(rimraf);
var fs__default = /*#__PURE__*/_interopDefaultCompat(fs$1);
var path__default$1 = /*#__PURE__*/_interopDefaultCompat(path$1);
var logSymbols__default = /*#__PURE__*/_interopDefaultCompat(logSymbols);
var oneline__default = /*#__PURE__*/_interopDefaultCompat(oneline);
var url__default = /*#__PURE__*/_interopDefaultCompat(url);
var EventSource__default = /*#__PURE__*/_interopDefaultCompat(EventSource);
var exportDataset__default = /*#__PURE__*/_interopDefaultCompat(exportDataset);
var fs__default$1 = /*#__PURE__*/_interopDefaultCompat(fs$2);
var sanityImport__default = /*#__PURE__*/_interopDefaultCompat(sanityImport);
var padStart__default = /*#__PURE__*/_interopDefaultCompat(padStart);
var chokidar__default = /*#__PURE__*/_interopDefaultCompat(chokidar);
var execa__default = /*#__PURE__*/_interopDefaultCompat(execa);
var json5__default = /*#__PURE__*/_interopDefaultCompat(json5);
var isEqual__default = /*#__PURE__*/_interopDefaultCompat(isEqual);
var isPlainObject__default = /*#__PURE__*/_interopDefaultCompat(isPlainObject);
var noop__default = /*#__PURE__*/_interopDefaultCompat(noop$1);
var os__default = /*#__PURE__*/_interopDefaultCompat(os);
var pluralize__default = /*#__PURE__*/_interopDefaultCompat(pluralize);
var tokenize__default = /*#__PURE__*/_interopDefaultCompat(tokenize);
var open__default = /*#__PURE__*/_interopDefaultCompat(open);
var groupBy__default = /*#__PURE__*/_interopDefaultCompat(groupBy);
var deburr__default = /*#__PURE__*/_interopDefaultCompat(deburr);
var size__default = /*#__PURE__*/_interopDefaultCompat(size);
var sortBy__default = /*#__PURE__*/_interopDefaultCompat(sortBy);
const defaultApiVersion$1 = "v2024-02-21";
const datasetBackupGroup = {
  name: "backup",
  signature: "[COMMAND]",
  description: "Manage dataset backups.",
  isGroupRoot: true,
  hideFromHelp: true
};
function parseApiErr(err) {
  var _a, _b, _c, _d;
  const apiErr = {};
  if (err.code) {
    apiErr.statusCode = err.code;
  } else if (err.statusCode) {
    apiErr.statusCode = err.statusCode;
  }
  if (err.message) {
    apiErr.message = err.message;
  } else if (err.statusMessage) {
    apiErr.message = err.statusMessage;
  } else if ((_b = (_a = err == null ? void 0 : err.response) == null ? void 0 : _a.body) == null ? void 0 : _b.message) {
    apiErr.message = err.response.body.message;
  } else if ((_d = (_c = err == null ? void 0 : err.response) == null ? void 0 : _c.data) == null ? void 0 : _d.message) {
    apiErr.message = err.response.data.message;
  } else {
    apiErr.message = JSON.stringify(err);
  }
  return apiErr;
}
const debug$2 = debug__default.default("sanity:core");
const MAX_DATASET_NAME_LENGTH$1 = 64;
function validateDatasetName(datasetName) {
  if (!datasetName) {
    return "Dataset name is missing";
  }
  const name = "".concat(datasetName);
  if (name.toLowerCase() !== name) {
    return "Dataset name must be all lowercase characters";
  }
  if (name.length < 2) {
    return "Dataset name must be at least two characters long";
  }
  if (name.length > MAX_DATASET_NAME_LENGTH$1) {
    return "Dataset name must be at most ".concat(MAX_DATASET_NAME_LENGTH$1, " characters");
  }
  if (!/^[a-z0-9]/.test(name)) {
    return "Dataset name must start with a letter or a number";
  }
  if (!/^[a-z0-9][-_a-z0-9]+$/.test(name)) {
    return "Dataset name must only contain letters, numbers, dashes and underscores";
  }
  if (/[-_]$/.test(name)) {
    return "Dataset name must not end with a dash or an underscore";
  }
  return false;
}
function promptForDatasetName(prompt) {
  let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
  return prompt.single({
    type: "input",
    message: "Dataset name:",
    validate: name => {
      const err = validateDatasetName(name);
      if (err) {
        return err;
      }
      return true;
    },
    ...options
  });
}
async function chooseDatasetPrompt(context) {
  let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
  const {
    apiClient,
    prompt
  } = context;
  const {
    message,
    allowCreation
  } = options;
  const client = apiClient();
  const datasets = await client.datasets.list();
  const hasProduction = datasets.find(dataset => dataset.name === "production");
  const datasetChoices = datasets.map(dataset => ({
    value: dataset.name
  }));
  const selected = await prompt.single({
    message: message || "Select dataset to use",
    type: "list",
    choices: allowCreation ? [{
      value: "new",
      name: "Create new dataset"
    }, new prompt.Separator(), ...datasetChoices] : datasetChoices
  });
  if (selected === "new") {
    debug$2("User wants to create a new dataset, prompting for name");
    const newDatasetName = await promptForDatasetName(prompt, {
      message: "Name your dataset:",
      default: hasProduction ? void 0 : "production"
    });
    await client.datasets.create(newDatasetName);
    return newDatasetName;
  }
  return selected;
}
async function resolveApiClient(context, datasetName, apiVersion) {
  const {
    apiClient
  } = context;
  let client = apiClient();
  const {
    projectId,
    token
  } = client.config();
  if (!projectId) {
    throw new Error("Project ID not defined");
  }
  let selectedDataset = datasetName;
  if (!selectedDataset) {
    selectedDataset = await chooseDatasetPrompt(context, {
      message: "Select the dataset name:"
    });
  }
  client = client.withConfig({
    dataset: datasetName,
    apiVersion
  });
  return {
    projectId,
    datasetName: selectedDataset,
    token,
    client
  };
}
const helpText$x = "\nExamples\n  sanity backup disable DATASET_NAME\n";
const disableDatasetBackupCommand = {
  name: "disable",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "Disable backup for a dataset.",
  helpText: helpText$x,
  action: async (args, context) => {
    const {
      output,
      chalk
    } = context;
    const [dataset] = args.argsWithoutOptions;
    const {
      projectId,
      datasetName,
      token,
      client
    } = await resolveApiClient(context, dataset, defaultApiVersion$1);
    try {
      await client.request({
        method: "PUT",
        headers: {
          Authorization: "Bearer ".concat(token)
        },
        uri: "/projects/".concat(projectId, "/datasets/").concat(datasetName, "/settings/backups"),
        body: {
          enabled: false
        }
      });
      output.print("".concat(chalk.green("Disabled daily backups for dataset ".concat(datasetName, "\n"))));
    } catch (error) {
      const {
        message
      } = parseApiErr(error);
      output.print("".concat(chalk.red("Disabling dataset backup failed: ".concat(message)), "\n"));
    }
  }
};
var debug$1 = require("debug")("sanity:backup");
const archiver = require("archiver");
function archiveDir(tmpOutDir, outFilePath, progressCb) {
  return new Promise((resolve, reject) => {
    const archiveDestination = node_fs.createWriteStream(outFilePath);
    archiveDestination.on("error", err => {
      reject(err);
    });
    archiveDestination.on("close", () => {
      resolve();
    });
    const archive = archiver("tar", {
      gzip: true,
      gzipOptions: {
        level: zlib__default.default.constants.Z_DEFAULT_COMPRESSION
      }
    });
    archive.on("error", err => {
      debug$1("Archiving errored!\n%s", err.stack);
      reject(err);
    });
    archive.on("warning", err => {
      debug$1("Archive warning: %s", err.message);
    });
    archive.on("progress", progress => {
      progressCb(progress.fs.processedBytes);
    });
    archive.pipe(archiveDestination);
    archive.directory(tmpOutDir, false);
    archive.finalize();
  });
}
const maxBackupIdsShown = 100;
async function chooseBackupIdPrompt(context, datasetName) {
  var _a;
  const {
    prompt
  } = context;
  const {
    projectId,
    token,
    client
  } = await resolveApiClient(context, datasetName, defaultApiVersion$1);
  try {
    const response = await client.request({
      headers: {
        Authorization: "Bearer ".concat(token)
      },
      uri: "/projects/".concat(projectId, "/datasets/").concat(datasetName, "/backups"),
      query: {
        limit: maxBackupIdsShown.toString()
      }
    });
    if (((_a = response == null ? void 0 : response.backups) == null ? void 0 : _a.length) > 0) {
      const backupIdChoices = response.backups.map(backup => ({
        value: backup.id
      }));
      const selected = await prompt.single({
        message: "Select backup ID to use (only last ".concat(maxBackupIdsShown, " shown)"),
        type: "list",
        choices: backupIdChoices
      });
      return selected;
    }
  } catch (err) {
    throw new Error("Failed to fetch backups for dataset ".concat(datasetName, ": ").concat(err.message));
  }
  throw new Error("No backups found");
}
function cleanupTmpDir(tmpDir) {
  rimraf__default.default(tmpDir, err => {
    if (err) {
      debug$1("Error cleaning up temporary files: ".concat(err.message));
    }
  });
}
const MAX_RETRIES = 5;
const BACKOFF_DELAY_BASE = 200;
const exponentialBackoff = retryCount => Math.pow(2, retryCount) * BACKOFF_DELAY_BASE;
async function withRetry(operation) {
  let maxRetries = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : MAX_RETRIES;
  for (let retryCount = 0; retryCount < maxRetries; retryCount++) {
    try {
      return await operation();
    } catch (err) {
      if (err.response && err.response.statusCode && err.response.statusCode < 500) {
        throw err;
      }
      const retryDelay = exponentialBackoff(retryCount);
      debug$1("Error encountered, retrying after ".concat(retryDelay, "ms: %s"), err.message);
      await new Promise(resolve => setTimeout(resolve, retryDelay));
    }
  }
  throw new Error("Operation failed after all retries");
}
const CONNECTION_TIMEOUT$1 = 15 * 1e3;
const READ_TIMEOUT$1 = 3 * 60 * 1e3;
const request$1 = getIt.getIt([middleware.keepAlive(), middleware.promise()]);
async function downloadAsset(url, fileName, fileType, outDir) {
  const normalizedFileName = path__default.default.basename(fileName);
  const assetFilePath = getAssetFilePath(normalizedFileName, fileType, outDir);
  await withRetry(async () => {
    const response = await request$1({
      url,
      maxRedirects: 5,
      timeout: {
        connect: CONNECTION_TIMEOUT$1,
        socket: READ_TIMEOUT$1
      },
      stream: true
    });
    debug$1("Received asset %s with status code %d", normalizedFileName, response == null ? void 0 : response.statusCode);
    await promises.pipeline(response.body, node_fs.createWriteStream(assetFilePath));
  });
}
function getAssetFilePath(fileName, fileType, outDir) {
  let assetFilePath = "";
  if (fileType === "image") {
    assetFilePath = path__default.default.join(outDir, "images", fileName);
  } else if (fileType === "file") {
    assetFilePath = path__default.default.join(outDir, "files", fileName);
  }
  return assetFilePath;
}
const CONNECTION_TIMEOUT = 15 * 1e3;
const READ_TIMEOUT = 3 * 60 * 1e3;
const request = getIt.getIt([middleware.keepAlive(), middleware.promise()]);
async function downloadDocument(url) {
  const response = await withRetry(() => request({
    url,
    maxRedirects: 5,
    timeout: {
      connect: CONNECTION_TIMEOUT,
      socket: READ_TIMEOUT
    }
  }));
  debug$1("Received document from %s with status code %d", url, response == null ? void 0 : response.statusCode);
  return response.body;
}
class PaginatedGetBackupStream extends node_stream.Readable {
  constructor(client, projectId, datasetName, backupId, token) {
    super({
      objectMode: true
    });
    this.cursor = "";
    this.totalFiles = 0;
    this.client = client;
    this.projectId = projectId;
    this.datasetName = datasetName;
    this.backupId = backupId;
    this.token = token;
  }
  async _read() {
    try {
      const data = await this.fetchNextBackupPage();
      if (this.totalFiles === 0) {
        this.totalFiles = data.totalFiles;
      }
      data.files.forEach(file => this.push(file));
      if (typeof data.nextCursor === "string" && data.nextCursor !== "") {
        this.cursor = data.nextCursor;
      } else {
        this.push(null);
      }
    } catch (err) {
      this.destroy(err);
    }
  }
  // fetchNextBackupPage fetches the next page of backed up files from the backup API.
  async fetchNextBackupPage() {
    const query = this.cursor === "" ? {} : {
      nextCursor: this.cursor
    };
    try {
      return await this.client.request({
        headers: {
          Authorization: "Bearer ".concat(this.token)
        },
        uri: "/projects/".concat(this.projectId, "/datasets/").concat(this.datasetName, "/backups/").concat(this.backupId),
        query
      });
    } catch (error) {
      let msg = error.statusCode ? error.response.body.message : error.message;
      if (msg === void 0) {
        msg = String(error);
      }
      throw new Error("Downloading dataset backup failed: ".concat(msg));
    }
  }
}
const newProgress = (output, startStep) => {
  let spinner = output.spinner(startStep).start();
  let lastProgress = {
    step: startStep
  };
  let start = Date.now();
  const print = progress => {
    const elapsed = prettyMs__default.default(Date.now() - start);
    if (progress.current && progress.current > 0 && progress.total && progress.total > 0) {
      spinner.text = "".concat(progress.step, " (").concat(progress.current, "/").concat(progress.total, ") [").concat(elapsed, "]");
    } else {
      spinner.text = "".concat(progress.step, " [").concat(elapsed, "]");
    }
  };
  return {
    set: progress => {
      if (progress.step !== lastProgress.step) {
        print(lastProgress);
        spinner.succeed();
        spinner = output.spinner(progress.step).start();
        start = Date.now();
      } else if (progress.step === lastProgress.step && progress.update) {
        print(progress);
      }
      lastProgress = progress;
    },
    update: progress => {
      print(progress);
      lastProgress = progress;
    },
    succeed: () => {
      spinner.succeed();
      start = Date.now();
    },
    fail: () => {
      spinner.fail();
      start = Date.now();
    }
  };
};
function humanFileSize(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return "".concat((size / Math.pow(1024, i)).toFixed(2), " ").concat(["B", "kB", "MB", "GB", "TB"][i]);
}
function isPathDirName(filepath) {
  return !/\.\w+$/.test(filepath);
}
const debug = debug__default.default("sanity:backup");
const DEFAULT_DOWNLOAD_CONCURRENCY = 10;
const MAX_DOWNLOAD_CONCURRENCY = 24;
const helpText$w = "\nOptions\n  --backup-id <string> The backup ID to download. (required)\n  --out <string>       The file or directory path the backup should download to.\n  --overwrite          Allows overwriting of existing backup file.\n  --concurrency <num>  Concurrent number of backup item downloads. (max: 24)\n\nExamples\n  sanity backup download DATASET_NAME --backup-id 2024-01-01-backup-1\n  sanity backup download DATASET_NAME --backup-id 2024-01-01-backup-2 --out /path/to/file\n  sanity backup download DATASET_NAME --backup-id 2024-01-01-backup-3 --out /path/to/file --overwrite\n";
function parseCliFlags$7(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).options("backup-id", {
    type: "string"
  }).options("out", {
    type: "string"
  }).options("concurrency", {
    type: "number",
    default: DEFAULT_DOWNLOAD_CONCURRENCY
  }).options("overwrite", {
    type: "boolean",
    default: false
  }).argv;
}
const downloadBackupCommand = {
  name: "download",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "Download a dataset backup to a local file.",
  helpText: helpText$w,
  // eslint-disable-next-line max-statements
  action: async (args, context) => {
    const {
      output,
      chalk
    } = context;
    const [client, opts] = await prepareBackupOptions(context, args);
    const {
      projectId,
      datasetName,
      backupId,
      outDir,
      outFileName
    } = opts;
    if (outDir === "" || outFileName === "") {
      output.print("Operation cancelled.");
      return;
    }
    const outFilePath = path__default.default.join(outDir, outFileName);
    output.print("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E");
    output.print("\u2502                                                           \u2502");
    output.print("\u2502 Downloading backup for:                                   \u2502");
    output.print("\u2502 ".concat(chalk.bold("projectId"), ": ").concat(chalk.cyan(projectId).padEnd(56), " \u2502"));
    output.print("\u2502 ".concat(chalk.bold("dataset"), ": ").concat(chalk.cyan(datasetName).padEnd(58), " \u2502"));
    output.print("\u2502 ".concat(chalk.bold("backupId"), ": ").concat(chalk.cyan(backupId).padEnd(56), " \u2502"));
    output.print("\u2502                                                           \u2502");
    output.print("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F");
    output.print("");
    output.print('Downloading backup to "'.concat(chalk.cyan(outFilePath), '"'));
    const start = Date.now();
    const progressSpinner = newProgress(output, "Setting up backup environment...");
    const tmpOutDir = await promises$1.mkdtemp(path__default.default.join(node_os.tmpdir(), "sanity-backup-"));
    for (const dir of [outDir, path__default.default.join(tmpOutDir, "images"), path__default.default.join(tmpOutDir, "files")]) {
      node_fs.mkdirSync(dir, {
        recursive: true
      });
    }
    debug("Writing to temporary directory %s", tmpOutDir);
    const tmpOutDocumentsFile = path__default.default.join(tmpOutDir, "data.ndjson");
    const docOutStream = node_fs.createWriteStream(tmpOutDocumentsFile);
    const docWriteMutex = new asyncMutex.Mutex();
    try {
      const backupFileStream = new PaginatedGetBackupStream(client, opts.projectId, opts.datasetName, opts.backupId, opts.token);
      const files = [];
      let i = 0;
      for await (const file of backupFileStream) {
        files.push(file);
        i++;
        progressSpinner.set({
          step: "Reading backup files...",
          update: true,
          current: i,
          total: backupFileStream.totalFiles
        });
      }
      let totalItemsDownloaded = 0;
      const {
        default: pMap
      } = await import('p-map');
      await pMap(files, async file => {
        if (file.type === "file" || file.type === "image") {
          await downloadAsset(file.url, file.name, file.type, tmpOutDir);
        } else {
          const doc = await downloadDocument(file.url);
          await docWriteMutex.runExclusive(() => {
            docOutStream.write("".concat(doc, "\n"));
          });
        }
        totalItemsDownloaded += 1;
        progressSpinner.set({
          step: "Downloading documents and assets...",
          update: true,
          current: totalItemsDownloaded,
          total: backupFileStream.totalFiles
        });
      }, {
        concurrency: opts.concurrency
      });
    } catch (error) {
      progressSpinner.fail();
      const {
        message
      } = parseApiErr(error);
      throw new Error("Downloading dataset backup failed: ".concat(message));
    }
    docOutStream.end();
    await promises.finished(docOutStream);
    progressSpinner.set({
      step: "Archiving files into a tarball...",
      update: true
    });
    try {
      await archiveDir(tmpOutDir, outFilePath, processedBytes => {
        progressSpinner.update({
          step: "Archiving files into a tarball, ".concat(humanFileSize(processedBytes), " bytes written...")
        });
      });
    } catch (err) {
      progressSpinner.fail();
      throw new Error("Archiving backup failed: ".concat(err.message));
    }
    progressSpinner.set({
      step: "Cleaning up temporary files at ".concat(chalk.cyan("".concat(tmpOutDir)))
    });
    cleanupTmpDir(tmpOutDir);
    progressSpinner.set({
      step: "Backup download complete [".concat(prettyMs__default.default(Date.now() - start), "]")
    });
    progressSpinner.succeed();
  }
};
async function prepareBackupOptions(context, args) {
  const flags = await parseCliFlags$7(args);
  const [dataset] = args.argsWithoutOptions;
  const {
    prompt,
    workDir
  } = context;
  const {
    projectId,
    datasetName,
    client
  } = await resolveApiClient(context, dataset, defaultApiVersion$1);
  const {
    token
  } = client.config();
  if (!isString__default.default(token) || token.length < 1) {
    throw new Error("token is missing");
  }
  if (!isString__default.default(datasetName) || datasetName.length < 1) {
    throw new Error("dataset ".concat(datasetName, " must be a valid dataset name"));
  }
  const backupId = String(flags["backup-id"] || (await chooseBackupIdPrompt(context, datasetName)));
  if (backupId.length < 1) {
    throw new Error("backup-id ".concat(flags["backup-id"], " should be a valid string"));
  }
  if ("concurrency" in flags) {
    if (flags.concurrency < 1 || flags.concurrency > MAX_DOWNLOAD_CONCURRENCY) {
      throw new Error("concurrency should be in 1 to ".concat(MAX_DOWNLOAD_CONCURRENCY, " range"));
    }
  }
  const defaultOutFileName = "".concat(datasetName, "-backup-").concat(backupId, ".tar.gz");
  let out = await (async () => {
    if (flags.out !== void 0) {
      return fs.absolutify(flags.out);
    }
    const input = await prompt.single({
      type: "input",
      message: "Output path:",
      default: path__default.default.join(workDir, defaultOutFileName),
      filter: fs.absolutify
    });
    return input;
  })();
  if (isPathDirName(out)) {
    out = path__default.default.join(out, defaultOutFileName);
  }
  if (!flags.overwrite && node_fs.existsSync(out)) {
    const shouldOverwrite = await prompt.single({
      type: "confirm",
      message: 'File "'.concat(out, '" already exists, would you like to overwrite it?'),
      default: false
    });
    if (!shouldOverwrite) {
      out = "";
    }
  }
  return [client, {
    projectId,
    datasetName,
    backupId,
    token,
    outDir: path__default.default.dirname(out),
    outFileName: path__default.default.basename(out),
    overwrite: flags.overwrite,
    concurrency: flags.concurrency || DEFAULT_DOWNLOAD_CONCURRENCY
  }];
}
const helpText$v = "\nExamples\n  sanity backup enable DATASET_NAME\n";
const enableDatasetBackupCommand = {
  name: "enable",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "Enable backup for a dataset.",
  helpText: helpText$v,
  action: async (args, context) => {
    const {
      output,
      chalk
    } = context;
    const [dataset] = args.argsWithoutOptions;
    const {
      projectId,
      datasetName,
      token,
      client
    } = await resolveApiClient(context, dataset, defaultApiVersion$1);
    try {
      await client.request({
        method: "PUT",
        headers: {
          Authorization: "Bearer ".concat(token)
        },
        uri: "/projects/".concat(projectId, "/datasets/").concat(datasetName, "/settings/backups"),
        body: {
          enabled: true
        }
      });
      output.print("".concat(chalk.green("Enabled backups for dataset ".concat(datasetName, ".\nPlease note that it may take up to 24 hours before the first backup is created.\n"))));
      output.print("".concat(chalk.bold("Retention policies may apply depending on your plan and agreement.\n")));
    } catch (error) {
      const {
        message
      } = parseApiErr(error);
      output.print("".concat(chalk.red("Enabling dataset backup failed: ".concat(message)), "\n"));
    }
  }
};
const DEFAULT_LIST_BACKUP_LIMIT = 30;
const helpText$u = "\nOptions\n  --limit <int>     Maximum number of backups returned. Default 30.\n  --after <string>  Only return backups after this date (inclusive)\n  --before <string> Only return backups before this date (exclusive). Cannot be younger than <after> if specified.\n\nExamples\n  sanity backup list DATASET_NAME\n  sanity backup list DATASET_NAME --limit 50\n  sanity backup list DATASET_NAME --after 2024-01-31 --limit 10\n  sanity backup list DATASET_NAME --after 2024-01-31 --before 2024-01-10\n";
function parseCliFlags$6(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).options("after", {
    type: "string"
  }).options("before", {
    type: "string"
  }).options("limit", {
    type: "number",
    default: DEFAULT_LIST_BACKUP_LIMIT,
    alias: "l"
  }).argv;
}
const listDatasetBackupCommand = {
  name: "list",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "List available backups for a dataset.",
  helpText: helpText$u,
  action: async (args, context) => {
    const {
      output,
      chalk
    } = context;
    const flags = await parseCliFlags$6(args);
    const [dataset] = args.argsWithoutOptions;
    const {
      projectId,
      datasetName,
      token,
      client
    } = await resolveApiClient(context, dataset, defaultApiVersion$1);
    const query = {
      limit: DEFAULT_LIST_BACKUP_LIMIT.toString()
    };
    if (flags.limit) {
      if (flags.limit < 1 || flags.limit > Number.MAX_SAFE_INTEGER) {
        throw new Error("Parsing --limit: must be an integer between 1 and ".concat(Number.MAX_SAFE_INTEGER));
      }
      query.limit = flags.limit.toString();
    }
    if (flags.before || flags.after) {
      try {
        const parsedBefore = processDateFlags(flags.before);
        const parsedAfter = processDateFlags(flags.after);
        if (parsedAfter && parsedBefore && dateFns.isAfter(parsedAfter, parsedBefore)) {
          throw new Error("--after date must be before --before");
        }
        query.before = flags.before;
        query.after = flags.after;
      } catch (err) {
        throw new Error("Parsing date flags: ".concat(err));
      }
    }
    let response;
    try {
      response = await client.request({
        headers: {
          Authorization: "Bearer ".concat(token)
        },
        uri: "/projects/".concat(projectId, "/datasets/").concat(datasetName, "/backups"),
        query: {
          ...query
        }
      });
    } catch (error) {
      const {
        message
      } = parseApiErr(error);
      output.error("".concat(chalk.red("List dataset backup failed: ".concat(message)), "\n"));
    }
    if (response && response.backups) {
      if (response.backups.length === 0) {
        output.print("No backups found.");
        return;
      }
      const table = new consoleTablePrinter.Table({
        columns: [{
          name: "resource",
          title: "RESOURCE",
          alignment: "left"
        }, {
          name: "createdAt",
          title: "CREATED AT",
          alignment: "left"
        }, {
          name: "backupId",
          title: "BACKUP ID",
          alignment: "left"
        }]
      });
      response.backups.forEach(backup => {
        const {
          id,
          createdAt
        } = backup;
        table.addRow({
          resource: "Dataset",
          createdAt: dateFns.lightFormat(Date.parse(createdAt), "yyyy-MM-dd HH:mm:ss"),
          backupId: id
        });
      });
      table.printTable();
    }
  }
};
function processDateFlags(date) {
  if (!date) return void 0;
  const parsedDate = dateFns.parse(date, "yyyy-MM-dd", /* @__PURE__ */new Date());
  if (dateFns.isValid(parsedDate)) {
    return parsedDate;
  }
  throw new Error("Invalid ".concat(date, " date format. Use YYYY-MM-DD"));
}
const helpText$t = "\nOptions\n  --source-maps Enable source maps for built bundles (increases size of bundle)\n  --no-minify Skip minifying built JavaScript (speeds up build, increases size of bundle)\n  -y, --yes Use unattended mode, accepting defaults and using only flags for choices\n\nExamples\n  sanity build\n  sanity build --no-minify --source-maps\n";
const buildCommand = {
  name: "build",
  signature: "[OUTPUT_DIR]",
  description: "Builds the Sanity Studio configuration into a static bundle",
  action: async (args, context, overrides) => {
    const buildAction = await getBuildAction();
    return buildAction(args, context, overrides);
  },
  helpText: helpText$t
};
async function getBuildAction() {
  const mod = await Promise.resolve().then(function () {
    return require('./buildAction-BdD3ajZy.js');
  });
  return mod.default;
}
const checkCommand = {
  name: "check",
  signature: "",
  description: "[deprecated]",
  helpText: "",
  hideFromHelp: true,
  action: (_args, context) => {
    const {
      output
    } = context;
    output.print("`sanity check` is deprecated and no longer has any effect");
    return Promise.resolve();
  }
};
const configCheckCommand = {
  name: "configcheck",
  signature: "",
  description: "Checks if the required configuration files for plugins exists and are up to date",
  helpText: "",
  hideFromHelp: true,
  action: async (args, context) => {
    context.output.error("`sanity configcheck` is no longer required/used");
    return Promise.resolve();
  }
};
var __freeze$1 = Object.freeze;
var __defProp$1 = Object.defineProperty;
var __template$1 = (cooked, raw) => __freeze$1(__defProp$1(cooked, "raw", {
  value: __freeze$1(raw || cooked.slice())
}));
var _a$1, _b, _c, _d;
const wildcardReplacement = "a-wild-card-r3pl4c3m3n7-a";
const portReplacement = ":7777777";
async function addCorsOrigin(givenOrigin, flags, context) {
  const {
    apiClient,
    prompt,
    output
  } = context;
  const origin = await (givenOrigin ? filterAndValidateOrigin(givenOrigin) : promptForOrigin$1(prompt));
  const hasWildcard = origin.includes("*");
  if (hasWildcard && !(await promptForWildcardConfirmation(origin, context))) {
    return false;
  }
  const allowCredentials = typeof flags.credentials === "undefined" ? await promptForCredentials(hasWildcard, context) : Boolean(flags.credentials);
  if (givenOrigin !== origin) {
    output.print("Normalized origin to ".concat(origin));
  }
  const client = apiClient({
    requireUser: true,
    requireProject: true
  });
  await client.request({
    method: "POST",
    url: "/cors",
    body: {
      origin,
      allowCredentials
    },
    maxRedirects: 0
  });
  return true;
}
function promptForCredentials(hasWildcard, context) {
  const {
    prompt,
    output,
    chalk
  } = context;
  output.print("");
  if (hasWildcard) {
    output.print(oneline__default.default(_a$1 || (_a$1 = __template$1(["\n      ", "\n      We ", " recommend NOT allowing credentials\n      on origins containing wildcards. If you are logged in to a studio, people will\n      be able to send requests ", " to read and modify\n      data, from any matching origin. Please tread carefully!\n    "])), chalk.yellow("".concat(logSymbols__default.default.warning, " Warning:")), chalk.red(chalk.underline("HIGHLY")), chalk.underline("on your behalf")));
  } else {
    output.print(oneline__default.default(_b || (_b = __template$1(["\n      ", "\n      Should this origin be allowed to send requests using authentication tokens or\n      session cookies? Be aware that any script on this origin will be able to send\n      requests ", ' to read and modify data if you\n      are logged in to a Sanity studio. If this origin hosts a studio, you will need\n      this, otherwise you should probably answer "No" (n).\n    '])), chalk.yellow("".concat(logSymbols__default.default.warning, " Warning:")), chalk.underline("on your behalf")));
  }
  output.print("");
  return prompt.single({
    type: "confirm",
    message: oneline__default.default(_c || (_c = __template$1(["\n      Allow credentials to be sent from this origin? Please read the warning above.\n    "]))),
    default: false
  });
}
function promptForWildcardConfirmation(origin, context) {
  const {
    prompt,
    output,
    chalk
  } = context;
  output.print("");
  output.print(chalk.yellow("".concat(logSymbols__default.default.warning, " Warning: Examples of allowed origins:")));
  if (origin === "*") {
    output.print("- http://www.some-malicious.site");
    output.print("- https://not.what-you-were-expecting.com");
    output.print("- https://high-traffic-site.com");
    output.print("- http://192.168.1.1:8080");
  } else {
    output.print("- ".concat(origin.replace(/:\*/, ":1234").replace(/\*/g, "foo")));
    output.print("- ".concat(origin.replace(/:\*/, ":3030").replace(/\*/g, "foo.bar")));
  }
  output.print("");
  return prompt.single({
    type: "confirm",
    message: oneline__default.default(_d || (_d = __template$1(["\n      Using wildcards can be ", ".\n      Are you ", " you want to allow this origin?"])), chalk.red("risky"), chalk.underline("absolutely sure")),
    default: false
  });
}
function promptForOrigin$1(prompt) {
  return prompt.single({
    type: "input",
    message: "Origin (including protocol):",
    filter: filterOrigin,
    validate: origin => validateOrigin(origin, origin)
  });
}
function filterOrigin(origin) {
  if (origin === "*" || origin === "file:///*" || origin === "null") {
    return origin;
  }
  try {
    const example = origin.replace(/([^:])\*/g, "$1".concat(wildcardReplacement)).replace(/:\*/, portReplacement);
    const parsed = url__default.default.parse(example);
    let host = parsed.host || "";
    if (/^https?:$/.test(parsed.protocol || "")) {
      host = host.replace(/:(80|443)$/, "");
    }
    host = host.replace(portReplacement, ":*").replace(new RegExp(wildcardReplacement, "g"), "*");
    return "".concat(parsed.protocol, "//").concat(host);
  } catch (err) {
    return null;
  }
}
function validateOrigin(origin, givenOrigin) {
  if (origin === "*" || origin === "file:///*" || origin === "null") {
    return true;
  }
  try {
    url__default.default.parse(origin || 0);
    return true;
  } catch (err) {}
  if (/^file:\/\//.test(givenOrigin)) {
    return "Only a local file wildcard is currently allowed: file:///*";
  }
  return 'Invalid origin "'.concat(givenOrigin, '", must include protocol (https://some.host)');
}
function filterAndValidateOrigin(givenOrigin) {
  const origin = filterOrigin(givenOrigin);
  const result = validateOrigin(origin, givenOrigin);
  if (result !== true) {
    throw new Error(result);
  }
  if (!origin) {
    throw new Error("Invalid origin");
  }
  return origin;
}
const helpText$s = "\nOptions\n  --credentials Allow credentials (token/cookie) to be sent from this origin\n  --no-credentials Disallow credentials (token/cookie) to be sent from this origin\n\nExamples\n  sanity cors add\n  sanity cors add http://localhost:3000 --no-credentials\n";
const addCorsOriginCommand = {
  name: "add",
  group: "cors",
  signature: "[ORIGIN]",
  helpText: helpText$s,
  description: "Allow a new origin to use your project API through CORS",
  action: async (args, context) => {
    const {
      output
    } = context;
    const [origin] = args.argsWithoutOptions;
    if (!origin) {
      throw new Error("No origin specified, use `sanity cors add <origin-url>`");
    }
    const flags = args.extOptions;
    const isFile = fs__default.default.existsSync(path__default$1.default.join(process.cwd(), origin));
    if (isFile) {
      output.warn('Origin "'.concat(origin, '?" Remember to quote values (sanity cors add "*")'));
    }
    const success = await addCorsOrigin(origin, flags, context);
    if (success) {
      output.print("CORS origin added successfully");
    }
  }
};
const corsGroup = {
  name: "cors",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Configures CORS settings for Sanity projects"
};
const helpText$r = "\nExamples\n  sanity cors delete\n  sanity cors delete http://localhost:3000\n";
const deleteCorsOriginCommand = {
  name: "delete",
  group: "cors",
  signature: "[ORIGIN]",
  helpText: helpText$r,
  description: "Delete an existing CORS-origin from your project",
  action: async (args, context) => {
    const {
      output,
      apiClient
    } = context;
    const [origin] = args.argsWithoutOptions;
    const client = apiClient({
      requireUser: true,
      requireProject: true
    });
    const originId = await promptForOrigin(origin, context);
    try {
      await client.request({
        method: "DELETE",
        uri: "/cors/".concat(originId)
      });
      output.print("Origin deleted");
    } catch (err) {
      throw new Error("Origin deletion failed:\n".concat(err.message));
    }
  }
};
async function promptForOrigin(specified, context) {
  const specifiedOrigin = specified && specified.toLowerCase();
  const {
    prompt,
    apiClient
  } = context;
  const client = apiClient({
    requireUser: true,
    requireProject: true
  });
  const origins = await client.request({
    url: "/cors"
  });
  if (specifiedOrigin) {
    const selected = origins.filter(origin => origin.origin.toLowerCase() === specifiedOrigin)[0];
    if (!selected) {
      throw new Error('Origin "'.concat(specified, ' not found"'));
    }
    return selected.id;
  }
  const choices = origins.map(origin => ({
    value: origin.id,
    name: origin.origin
  }));
  return prompt.single({
    message: "Select origin to delete",
    type: "list",
    choices
  });
}
const helpText$q = "\nExamples\n  sanity cors list\n";
const listCorsOriginsCommand = {
  name: "list",
  group: "cors",
  signature: "",
  helpText: helpText$q,
  description: "List all origins allowed to access the API for this project",
  action: async (args, context) => {
    const {
      output
    } = context;
    const {
      apiClient
    } = context;
    const client = apiClient({
      requireUser: true,
      requireProject: true
    });
    const origins = await client.request({
      url: "/cors"
    });
    output.print(origins.map(origin => origin.origin).join("\n"));
  }
};
const MAX_DATASET_NAME_LENGTH = 64;
function validateDatasetAliasName(datasetName) {
  if (!datasetName) {
    return "Alias name is missing";
  }
  const name = "".concat(datasetName);
  if (name.toLowerCase() !== name) {
    return "Alias name must be all lowercase characters";
  }
  if (name.length < 2) {
    return "Alias name must be at least two characters long";
  }
  if (name.length > MAX_DATASET_NAME_LENGTH) {
    return "Alias name must be at most ".concat(MAX_DATASET_NAME_LENGTH, " characters");
  }
  if (!/^[a-z0-9~]/.test(name)) {
    return "Alias name must start with a letter or a number";
  }
  if (!/^[a-z0-9~][-_a-z0-9]+$/.test(name)) {
    return "Alias name must only contain letters, numbers, dashes and underscores";
  }
  if (/[-_]$/.test(name)) {
    return "Alias name must not end with a dash or an underscore";
  }
  return false;
}
function promptForDatasetAliasName(prompt) {
  let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
  return prompt.single({
    type: "input",
    message: "Alias name:",
    validate: name => {
      const err = validateDatasetAliasName(name);
      if (err) {
        return err;
      }
      return true;
    },
    ...options
  });
}
const ALIAS_PREFIX = "~";
function listAliases(client) {
  return client.request({
    uri: "/aliases"
  });
}
function createAlias(client, aliasName, datasetName) {
  return modify(client, "PUT", aliasName, datasetName ? {
    datasetName
  } : void 0);
}
function updateAlias(client, aliasName, datasetName) {
  return modify(client, "PATCH", aliasName, datasetName ? {
    datasetName
  } : void 0);
}
function unlinkAlias(client, aliasName) {
  return modify(client, "PATCH", "".concat(aliasName, "/unlink"), {});
}
function removeAlias(client, aliasName) {
  return modify(client, "DELETE", aliasName);
}
function modify(client, method, aliasName, body) {
  return client.request({
    method,
    uri: "/aliases/".concat(aliasName),
    body
  });
}
const createAliasHandler = async (args, context) => {
  const {
    apiClient,
    output,
    prompt
  } = context;
  const [, alias, targetDataset] = args.argsWithoutOptions;
  const client = apiClient();
  const nameError = alias && validateDatasetAliasName(alias);
  if (nameError) {
    throw new Error(nameError);
  }
  const [datasets, aliases, projectFeatures] = await Promise.all([client.datasets.list().then(sets => sets.map(ds => ds.name)), listAliases(client).then(sets => sets.map(ds => ds.name)), client.request({
    uri: "/features"
  })]);
  let aliasName = await (alias || promptForDatasetAliasName(prompt));
  let aliasOutputName = aliasName;
  if (aliasName.startsWith(ALIAS_PREFIX)) {
    aliasName = aliasName.substring(1);
  } else {
    aliasOutputName = "".concat(ALIAS_PREFIX).concat(aliasName);
  }
  if (aliases.includes(aliasName)) {
    throw new Error('Dataset alias "'.concat(aliasOutputName, '" already exists'));
  }
  if (targetDataset) {
    const datasetErr = validateDatasetName(targetDataset);
    if (datasetErr) {
      throw new Error(datasetErr);
    }
  }
  const datasetName = await (targetDataset || promptForDatasetName(prompt));
  if (datasetName && !datasets.includes(datasetName)) {
    throw new Error('Dataset "'.concat(datasetName, '" does not exist '));
  }
  const canCreateAlias = projectFeatures.includes("advancedDatasetManagement");
  if (!canCreateAlias) {
    throw new Error("This project cannot create a dataset alias");
  }
  try {
    await createAlias(client, aliasName, datasetName);
    output.print("Dataset alias ".concat(aliasOutputName, " created ").concat(datasetName && "and linked to ".concat(datasetName), " successfully"));
  } catch (err) {
    throw new Error("Dataset alias creation failed:\n".concat(err.message));
  }
};
function parseCliFlags$5(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("force", {
    type: "boolean"
  }).argv;
}
const deleteAliasHandler = async (args, context) => {
  const {
    apiClient,
    prompt,
    output
  } = context;
  const [, ds] = args.argsWithoutOptions;
  const {
    force
  } = await parseCliFlags$5(args);
  const client = apiClient();
  if (!ds) {
    throw new Error("Dataset alias name must be provided");
  }
  let aliasName = "".concat(ds);
  const dsError = validateDatasetAliasName(aliasName);
  if (dsError) {
    throw dsError;
  }
  aliasName = aliasName.startsWith(ALIAS_PREFIX) ? aliasName.substring(1) : aliasName;
  const [fetchedAliases] = await Promise.all([listAliases(client)]);
  const linkedAlias = fetchedAliases.find(elem => elem.name === aliasName);
  const message = linkedAlias && linkedAlias.datasetName ? "This dataset alias is linked to ".concat(linkedAlias.datasetName, ". ") : "";
  if (force) {
    output.warn("'--force' used: skipping confirmation, deleting alias \"".concat(aliasName, '"'));
  } else {
    await prompt.single({
      type: "input",
      message: "".concat(message, "Are you ABSOLUTELY sure you want to delete this dataset alias?\n  Type the name of the dataset alias to confirm delete: "),
      filter: input => "".concat(input).trim(),
      validate: input => {
        return input === aliasName || "Incorrect dataset alias name. Ctrl + C to cancel delete.";
      }
    });
  }
  return removeAlias(client, aliasName).then(() => {
    output.print("Dataset alias deleted successfully");
  });
};
const linkAliasHandler = async (args, context) => {
  const {
    apiClient,
    output,
    prompt
  } = context;
  const [, alias, targetDataset] = args.argsWithoutOptions;
  const flags = args.extOptions;
  const client = apiClient();
  const nameError = alias && validateDatasetAliasName(alias);
  if (nameError) {
    throw new Error(nameError);
  }
  const [datasets, fetchedAliases] = await Promise.all([client.datasets.list().then(sets => sets.map(ds => ds.name)), listAliases(client)]);
  const aliases = fetchedAliases.map(da => da.name);
  let aliasName = await (alias || promptForDatasetAliasName(prompt));
  let aliasOutputName = aliasName;
  if (aliasName.startsWith(ALIAS_PREFIX)) {
    aliasName = aliasName.substring(1);
  } else {
    aliasOutputName = "".concat(ALIAS_PREFIX).concat(aliasName);
  }
  if (!aliases.includes(aliasName)) {
    throw new Error('Dataset alias "'.concat(aliasOutputName, '" does not exist '));
  }
  const datasetName = await (targetDataset || promptForDatasetName(prompt));
  const datasetErr = validateDatasetName(datasetName);
  if (datasetErr) {
    throw new Error(datasetErr);
  }
  if (!datasets.includes(datasetName)) {
    throw new Error('Dataset "'.concat(datasetName, '" does not exist '));
  }
  const linkedAlias = fetchedAliases.find(elem => elem.name === aliasName);
  if (linkedAlias && linkedAlias.datasetName) {
    if (linkedAlias.datasetName === datasetName) {
      throw new Error("Dataset alias ".concat(aliasOutputName, " already linked to ").concat(datasetName));
    }
    if (!flags.force) {
      await prompt.single({
        type: "input",
        message: "This alias is linked to dataset <".concat(linkedAlias.datasetName, ">. Are you ABSOLUTELY sure you want to link this dataset alias to this dataset?\n        \n  Type YES/NO: "),
        filter: input => "".concat(input).toLowerCase(),
        validate: input => {
          return input === "yes" || "Ctrl + C to cancel dataset alias link.";
        }
      });
    }
  }
  try {
    await updateAlias(client, aliasName, datasetName);
    output.print("Dataset alias ".concat(aliasOutputName, " linked to ").concat(datasetName, " successfully"));
  } catch (err) {
    throw new Error("Dataset alias link failed:\n".concat(err.message));
  }
};
function parseCliFlags$4(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("force", {
    type: "boolean"
  }).argv;
}
const unlinkAliasHandler = async (args, context) => {
  const {
    apiClient,
    output,
    prompt
  } = context;
  const [, alias] = args.argsWithoutOptions;
  const {
    force
  } = await parseCliFlags$4(args);
  const client = apiClient();
  const nameError = alias && validateDatasetAliasName(alias);
  if (nameError) {
    throw new Error(nameError);
  }
  const fetchedAliases = await listAliases(client);
  let aliasName = await (alias || promptForDatasetAliasName(prompt));
  let aliasOutputName = aliasName;
  if (aliasName.startsWith(ALIAS_PREFIX)) {
    aliasName = aliasName.substring(1);
  } else {
    aliasOutputName = "".concat(ALIAS_PREFIX).concat(aliasName);
  }
  const linkedAlias = fetchedAliases.find(elem => elem.name === aliasName);
  if (!linkedAlias) {
    throw new Error('Dataset alias "'.concat(aliasOutputName, '" does not exist'));
  }
  if (!linkedAlias.datasetName) {
    throw new Error('Dataset alias "'.concat(aliasOutputName, '" is not linked to a dataset'));
  }
  if (force) {
    output.warn("'--force' used: skipping confirmation, unlinking alias \"".concat(aliasOutputName, '"'));
  } else {
    await prompt.single({
      type: "input",
      message: 'Are you ABSOLUTELY sure you want to unlink this alias from the "'.concat(linkedAlias.datasetName, '" dataset?\n        \n  Type YES/NO: '),
      filter: input => "".concat(input).toLowerCase(),
      validate: input => {
        return input === "yes" || "Ctrl + C to cancel dataset alias unlink.";
      }
    });
  }
  try {
    const result = await unlinkAlias(client, aliasName);
    output.print("Dataset alias ".concat(aliasOutputName, " unlinked from ").concat(result.datasetName, " successfully"));
  } catch (err) {
    throw new Error("Dataset alias unlink failed:\n".concat(err.message));
  }
};
var __freeze = Object.freeze;
var __defProp = Object.defineProperty;
var __template = (cooked, raw) => __freeze(__defProp(cooked, "raw", {
  value: __freeze(raw || cooked.slice())
}));
var _a;
const helpText$p = "\nBelow are examples of the alias subcommand\n\nCreate Alias\n  sanity dataset alias create\n  sanity dataset alias create <alias-name>\n  sanity dataset alias create <alias-name> <target-dataset>\n\nDelete Alias\n  Options\n    --force Skips security prompt and forces link command\n\n  Usage\n    sanity dataset alias delete <alias-name>\n    sanity dataset alias delete <alias-name> --force\n\nLink Alias\n  Options\n    --force Skips security prompt and forces link command\n\n  Usage\n    sanity dataset alias link\n    sanity dataset alias link <alias-name>\n    sanity dataset alias link <alias-name> <target-dataset>\n    sanity dataset alias link <alias-name> <target-dataset> --force\n\nUn-link Alias\n  Options\n    --force Skips security prompt and forces link command\n\n  Usage\n    sanity dataset alias unlink\n    sanity dataset alias unlink <alias-name>\n    sanity dataset alias unlink <alias-name> --force\n";
const aliasCommand = {
  name: "alias",
  group: "dataset",
  signature: "SUBCOMMAND [ALIAS_NAME, TARGET_DATASET]",
  helpText: helpText$p,
  description: "You can manage your dataset alias using this command.",
  action: async (args, context) => {
    const [verb] = args.argsWithoutOptions;
    switch (verb) {
      case "create":
        await createAliasHandler(args, context);
        break;
      case "delete":
        await deleteAliasHandler(args, context);
        break;
      case "unlink":
        await unlinkAliasHandler(args, context);
        break;
      case "link":
        await linkAliasHandler(args, context);
        break;
      default:
        throw new Error(oneline__default.default(_a || (_a = __template(["\n          Invalid command provided. Available commands are: create, delete, link and unlink.\n          For more guide run the help command 'sanity dataset alias --help'\n        "]))));
    }
  }
};
async function listDatasetCopyJobs(flags, context) {
  const {
    apiClient,
    output,
    chalk
  } = context;
  const client = apiClient();
  const projectId = client.config().projectId;
  const query = {};
  let response;
  if (flags.offset && flags.offset >= 0) {
    query.offset = "".concat(flags.offset);
  }
  if (flags.limit && flags.limit > 0) {
    query.limit = "".concat(flags.limit);
  }
  try {
    response = await client.request({
      method: "GET",
      uri: "/projects/".concat(projectId, "/datasets/copy"),
      query
    });
  } catch (error) {
    if (error.statusCode) {
      output.error("".concat(chalk.red("Dataset copy list failed:\n".concat(error.response.body.message)), "\n"));
    } else {
      output.error("".concat(chalk.red("Dataset copy list failed:\n".concat(error.message)), "\n"));
    }
  }
  if (response && response.length > 0) {
    const table = new consoleTablePrinter.Table({
      title: "Dataset copy jobs for this project in descending order",
      columns: [{
        name: "id",
        title: "Job ID",
        alignment: "left"
      }, {
        name: "sourceDataset",
        title: "Source Dataset",
        alignment: "left"
      }, {
        name: "targetDataset",
        title: "Target Dataset",
        alignment: "left"
      }, {
        name: "state",
        title: "State",
        alignment: "left"
      }, {
        name: "withHistory",
        title: "With history",
        alignment: "left"
      }, {
        name: "timeStarted",
        title: "Time started",
        alignment: "left"
      }, {
        name: "timeTaken",
        title: "Time taken",
        alignment: "left"
      }]
    });
    response.forEach(job => {
      const {
        id,
        state,
        createdAt,
        updatedAt,
        sourceDataset,
        targetDataset,
        withHistory
      } = job;
      let timeStarted = "";
      if (createdAt !== "") {
        timeStarted = dateFns.formatDistanceToNow(dateFns.parseISO(createdAt));
      }
      let timeTaken = "";
      if (updatedAt !== "") {
        timeTaken = dateFns.formatDistance(dateFns.parseISO(updatedAt), dateFns.parseISO(createdAt));
      }
      let color;
      switch (state) {
        case "completed":
          color = "green";
          break;
        case "failed":
          color = "red";
          break;
        case "pending":
          color = "yellow";
          break;
        default:
          color = "";
      }
      table.addRow({
        id,
        state,
        withHistory,
        timeStarted: "".concat(timeStarted, " ago"),
        timeTaken,
        sourceDataset,
        targetDataset
      }, {
        color
      });
    });
    table.printTable();
  } else {
    output.print("This project doesn't have any dataset copy jobs");
  }
}
const getClientUrl = function (client, uri) {
  let useCdn = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;
  const config = client.config();
  const base = useCdn ? config.cdnUrl : config.url;
  return "".concat(base, "/").concat(uri.replace(/^\//, ""));
};
const helpText$o = "\nOptions\n  --detach Start the copy without waiting for it to finish\n  --attach <job-id> Attach to the running copy process to show progress\n  --skip-history Don't preserve document history on copy\n  --list Lists all dataset copy jobs corresponding to a certain criteria.\n  --offset Start position in the list of jobs. Default 0. With --list.\n  --limit Maximum number of jobs returned. Default 10. Maximum 1000. With --list.\n\nExamples\n  sanity dataset copy\n  sanity dataset copy <source-dataset>\n  sanity dataset copy <source-dataset> <target-dataset>\n  sanity dataset copy --skip-history <source-dataset> <target-dataset>\n  sanity dataset copy --detach <source-dataset> <target-dataset>\n  sanity dataset copy --attach <job-id>\n  sanity dataset copy --list\n  sanity dataset copy --list --offset=2\n  sanity dataset copy --list --offset=2 --limit=10\n";
function parseCliFlags$3(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("attach", {
    type: "string"
  }).option("list", {
    type: "boolean"
  }).option("limit", {
    type: "number"
  }).option("offset", {
    type: "number"
  }).option("skip-history", {
    type: "boolean"
  }).option("detach", {
    type: "boolean"
  }).argv;
}
const progress = url => {
  return new rxjs.Observable(observer => {
    let progressSource = new EventSource__default.default(url);
    let stopped = false;
    function onError(error) {
      if (progressSource) {
        progressSource.close();
      }
      debug$2("Error received: ".concat(error));
      if (stopped) {
        return;
      }
      observer.next({
        type: "reconnect"
      });
      progressSource = new EventSource__default.default(url);
    }
    function onChannelError(error) {
      stopped = true;
      progressSource.close();
      observer.error(error);
    }
    function onMessage(event) {
      const data = JSON.parse(event.data);
      if (data.state === "failed") {
        debug$2("Job failed. Data: %o", event);
        observer.error(event);
      } else if (data.state === "completed") {
        debug$2("Job succeeded. Data: %o", event);
        onComplete();
      } else {
        debug$2("Job progressed. Data: %o", event);
        observer.next(data);
      }
    }
    function onComplete() {
      progressSource.removeEventListener("error", onError);
      progressSource.removeEventListener("channel_error", onChannelError);
      progressSource.removeEventListener("job", onMessage);
      progressSource.removeEventListener("done", onComplete);
      progressSource.close();
      observer.complete();
    }
    progressSource.addEventListener("error", onError);
    progressSource.addEventListener("channel_error", onChannelError);
    progressSource.addEventListener("job", onMessage);
    progressSource.addEventListener("done", onComplete);
  });
};
const followProgress = (jobId, client, output) => {
  let currentProgress = 0;
  const spinner = output.spinner({}).start();
  const listenUrl = getClientUrl(client, "jobs/".concat(jobId, "/listen"));
  debug$2("Listening to ".concat(listenUrl));
  return new Promise((resolve, reject) => {
    progress(listenUrl).subscribe({
      next: event => {
        if (typeof event.progress === "number") {
          currentProgress = event.progress;
        }
        spinner.text = "Copy in progress: ".concat(currentProgress, "%");
      },
      error: err => {
        spinner.fail();
        reject(new Error("".concat(err.data)));
      },
      complete: () => {
        spinner.succeed("Copy finished.");
        resolve();
      }
    });
  });
};
const copyDatasetCommand = {
  name: "copy",
  group: "dataset",
  signature: "[SOURCE_DATASET] [TARGET_DATASET]",
  helpText: helpText$o,
  description: "Manages dataset copying, including starting a new copy job, listing copy jobs and following the progress of a running copy job",
  action: async (args, context) => {
    const {
      apiClient,
      output,
      prompt,
      chalk
    } = context;
    const flags = await parseCliFlags$3(args);
    const client = apiClient();
    if (flags.list) {
      await listDatasetCopyJobs(flags, context);
      return;
    }
    if (flags.attach) {
      const jobId = flags.attach;
      if (!jobId) {
        throw new Error("Please supply a jobId");
      }
      await followProgress(jobId, client, output);
      return;
    }
    const [sourceDataset, targetDataset] = args.argsWithoutOptions;
    const shouldSkipHistory = Boolean(flags["skip-history"]);
    const nameError = sourceDataset && validateDatasetName(sourceDataset);
    if (nameError) {
      throw new Error(nameError);
    }
    const existingDatasets = await client.datasets.list().then(datasets => datasets.map(ds => ds.name));
    const sourceDatasetName = await (sourceDataset || promptForDatasetName(prompt, {
      message: "Source dataset name:"
    }));
    if (!existingDatasets.includes(sourceDatasetName)) {
      throw new Error('Source dataset "'.concat(sourceDatasetName, "\" doesn't exist"));
    }
    const targetDatasetName = await (targetDataset || promptForDatasetName(prompt, {
      message: "Target dataset name:"
    }));
    if (existingDatasets.includes(targetDatasetName)) {
      throw new Error('Target dataset "'.concat(targetDatasetName, '" already exists'));
    }
    const err = validateDatasetName(targetDatasetName);
    if (err) {
      throw new Error(err);
    }
    try {
      const response = await client.request({
        method: "PUT",
        uri: "/datasets/".concat(sourceDatasetName, "/copy"),
        body: {
          targetDataset: targetDatasetName,
          skipHistory: shouldSkipHistory
        }
      });
      output.print("Copying dataset ".concat(chalk.green(sourceDatasetName), " to ").concat(chalk.green(targetDatasetName), "..."));
      if (!shouldSkipHistory) {
        output.print("Note: You can run this command with flag '--skip-history'. The flag will reduce copy time in larger datasets.");
      }
      output.print("Job ".concat(chalk.green(response.jobId), " started"));
      if (flags.detach) {
        return;
      }
      await followProgress(response.jobId, client, output);
      output.print("Job ".concat(chalk.green(response.jobId), " completed"));
    } catch (error) {
      if (error.statusCode) {
        output.print("".concat(chalk.red("Dataset copying failed:\n".concat(error.response.body.message)), "\n"));
      } else {
        output.print("".concat(chalk.red("Dataset copying failed:\n".concat(error.message)), "\n"));
      }
    }
  }
};
const helpText$n = "\nOptions\n  --visibility <mode> Set visibility for this dataset (public/private)\n\nExamples\n  sanity dataset create\n  sanity dataset create <name>\n  sanity dataset create <name> --visibility private\n";
const allowedModes = ["private", "public", "custom"];
const createDatasetCommand = {
  name: "create",
  group: "dataset",
  signature: "[NAME]",
  helpText: helpText$n,
  description: "Create a new dataset within your project",
  action: async (args, context) => {
    const {
      apiClient,
      output,
      prompt
    } = context;
    const flags = args.extOptions;
    const [dataset] = args.argsWithoutOptions;
    const client = apiClient();
    const nameError = dataset && validateDatasetName(dataset);
    if (nameError) {
      throw new Error(nameError);
    }
    const [datasets, projectFeatures] = await Promise.all([client.datasets.list().then(sets => sets.map(ds => ds.name)), client.request({
      uri: "/features"
    })]);
    if (flags.visibility && !allowedModes.includes(flags.visibility)) {
      throw new Error('Visibility mode "'.concat(flags.visibility, '" not allowed'));
    }
    const datasetName = await (dataset || promptForDatasetName(prompt));
    if (datasets.includes(datasetName)) {
      throw new Error('Dataset "'.concat(datasetName, '" already exists'));
    }
    const canCreatePrivate = projectFeatures.includes("privateDataset");
    debug$2("%s create private datasets", canCreatePrivate ? "Can" : "Cannot");
    const defaultAclMode = canCreatePrivate ? flags.visibility : "public";
    const aclMode = await (defaultAclMode || promptForDatasetVisibility(prompt, output));
    try {
      await client.datasets.create(datasetName, {
        aclMode
      });
      output.print("Dataset created successfully");
    } catch (err) {
      throw new Error("Dataset creation failed:\n".concat(err.message));
    }
  }
};
async function promptForDatasetVisibility(prompt, output) {
  const mode = await prompt.single({
    type: "list",
    message: "Dataset visibility",
    choices: [{
      value: "public",
      name: "Public (world readable)"
    }, {
      value: "private",
      name: "Private (Authenticated user or token needed)"
    }]
  });
  if (mode === "private") {
    output.print("Please note that while documents are private, assets (files and images) are still public\n");
  }
  return mode;
}
var datasetGroup = {
  name: "dataset",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Manages datasets, like create or delete, within projects"
};
const datasetVisibilityCommand = {
  name: "visibility",
  group: "dataset",
  helpText: "",
  signature: "get/set [dataset] [mode]",
  description: "Set visibility of a dataset",
  action: async (args, context) => {
    const {
      apiClient,
      output
    } = context;
    const [action, ds, aclMode] = args.argsWithoutOptions;
    const client = apiClient();
    if (!client.datasets.edit) {
      throw new Error("@sanity/cli must be upgraded first:\n  npm install -g @sanity/cli");
    }
    if (!action) {
      throw new Error("Action must be provided (get/set)");
    }
    if (!["set", "get"].includes(action)) {
      throw new Error("Invalid action (only get/set allowed)");
    }
    if (!ds) {
      throw new Error("Dataset name must be provided");
    }
    if (action === "set" && !aclMode) {
      throw new Error("Please provide a visibility mode (public/private)");
    }
    const dataset = "".concat(ds);
    const dsError = validateDatasetName(dataset);
    if (dsError) {
      throw new Error(dsError);
    }
    const current = (await client.datasets.list()).find(curr => curr.name === dataset);
    if (!current) {
      throw new Error("Dataset not found");
    }
    if (action === "get") {
      output.print(current.aclMode);
      return;
    }
    if (current.aclMode === aclMode) {
      output.print('Dataset already in "'.concat(aclMode, '"-mode'));
      return;
    }
    if (aclMode === "private") {
      output.print("Please note that while documents are private, assets (files and images) are still public\n");
    }
    await client.datasets.edit(dataset, {
      aclMode
    });
    output.print("Dataset visibility changed");
  }
};
const helpText$m = "\nOptions\n  --force Do not prompt for delete confirmation - forcefully delete\n\nExamples\n  sanity dataset delete\n  sanity dataset delete my-dataset\n  sanity dataset delete my-dataset --force\n";
function parseCliFlags$2(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("force", {
    type: "boolean"
  }).argv;
}
const deleteDatasetCommand = {
  name: "delete",
  group: "dataset",
  helpText: helpText$m,
  signature: "[datasetName]",
  description: "Delete a dataset within your project",
  action: async (args, context) => {
    const {
      apiClient,
      prompt,
      output
    } = context;
    const {
      force
    } = await parseCliFlags$2(args);
    const [ds] = args.argsWithoutOptions;
    if (!ds) {
      throw new Error("Dataset name must be provided");
    }
    const dataset = "".concat(ds);
    const dsError = validateDatasetName(dataset);
    if (dsError) {
      throw dsError;
    }
    if (force) {
      output.warn("'--force' used: skipping confirmation, deleting dataset \"".concat(dataset, '"'));
    } else {
      await prompt.single({
        type: "input",
        message: "Are you ABSOLUTELY sure you want to delete this dataset?\n  Type the name of the dataset to confirm delete:",
        filter: input => "".concat(input).trim(),
        validate: input => {
          return input === dataset || "Incorrect dataset name. Ctrl + C to cancel delete.";
        }
      });
    }
    await apiClient().datasets.delete(dataset);
    output.print("Dataset deleted successfully");
  }
};
const noop = () => null;
const helpText$l = "\nOptions\n  --raw                     Extract only documents, without rewriting asset references\n  --no-assets               Export only non-asset documents and remove references to image assets\n  --no-drafts               Export only published versions of documents\n  --no-compress             Skips compressing tarball entries (still generates a gzip file)\n  --types                   Defines which document types to export\n  --overwrite               Overwrite any file with the same name\n  --asset-concurrency <num> Concurrent number of asset downloads\n\nExamples\n  sanity dataset export moviedb localPath.tar.gz\n  sanity dataset export moviedb assetless.tar.gz --no-assets\n  sanity dataset export staging staging.tar.gz --raw\n  sanity dataset export staging staging.tar.gz --types products,shops\n";
function parseFlags$1(rawFlags) {
  const flags = {};
  if (rawFlags.types) {
    flags.types = "".concat(rawFlags.types).split(",");
  }
  if (rawFlags["asset-concurrency"]) {
    flags.assetConcurrency = parseInt(rawFlags["asset-concurrency"], 10);
  }
  if (typeof rawFlags.raw !== "undefined") {
    flags.raw = Boolean(rawFlags.raw);
  }
  if (typeof rawFlags.assets !== "undefined") {
    flags.assets = Boolean(rawFlags.assets);
  }
  if (typeof rawFlags.drafts !== "undefined") {
    flags.drafts = Boolean(rawFlags.drafts);
  }
  if (typeof rawFlags.compress !== "undefined") {
    flags.compress = Boolean(rawFlags.compress);
  }
  if (typeof rawFlags.overwrite !== "undefined") {
    flags.overwrite = Boolean(rawFlags.overwrite);
  }
  return flags;
}
const exportDatasetCommand = {
  name: "export",
  group: "dataset",
  signature: "[NAME] [DESTINATION]",
  description: "Export dataset to local filesystem as a gzipped tarball",
  helpText: helpText$l,
  action: async (args, context) => {
    const {
      apiClient,
      output,
      chalk,
      workDir,
      prompt
    } = context;
    const client = apiClient();
    const [targetDataset, targetDestination] = args.argsWithoutOptions;
    const flags = parseFlags$1(args.extOptions);
    let dataset = targetDataset ? "".concat(targetDataset) : null;
    if (!dataset) {
      dataset = await chooseDatasetPrompt(context, {
        message: "Select dataset to export"
      });
    }
    const dsError = validateDatasetName(dataset);
    if (dsError) {
      throw dsError;
    }
    const datasets = await client.datasets.list();
    if (!datasets.find(set => set.name === dataset)) {
      throw new Error('Dataset with name "'.concat(dataset, '" not found'));
    }
    const {
      projectId
    } = client.config();
    output.print("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E");
    output.print("\u2502                                               \u2502");
    output.print("\u2502 Exporting from:                               \u2502");
    output.print("\u2502 ".concat(chalk.bold("projectId"), ": ").concat(chalk.cyan(projectId).padEnd(44), " \u2502"));
    output.print("\u2502 ".concat(chalk.bold("dataset"), ": ").concat(chalk.cyan(dataset).padEnd(46), " \u2502"));
    output.print("\u2502                                               \u2502");
    output.print("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F");
    output.print("");
    let destinationPath = targetDestination;
    if (!destinationPath) {
      destinationPath = await prompt.single({
        type: "input",
        message: "Output path:",
        default: path__default$1.default.join(workDir, "".concat(dataset, ".tar.gz")),
        filter: fs.absolutify
      });
    }
    const outputPath = await getOutputPath(destinationPath, dataset, prompt, flags);
    if (!outputPath) {
      output.print("Cancelled");
      return;
    }
    if (outputPath !== "-") {
      output.print('Exporting dataset "'.concat(chalk.cyan(dataset), '" to "').concat(chalk.cyan(outputPath), '"'));
    }
    let currentStep = "Exporting documents...";
    let spinner = output.spinner(currentStep).start();
    const onProgress = progress => {
      if (progress.step !== currentStep) {
        spinner.succeed();
        spinner = output.spinner(progress.step).start();
      } else if (progress.step === currentStep && progress.update) {
        spinner.text = "".concat(progress.step, " (").concat(progress.current, "/").concat(progress.total, ")");
      }
      currentStep = progress.step;
    };
    const start = Date.now();
    try {
      await exportDataset__default.default({
        client,
        dataset,
        outputPath,
        onProgress,
        ...flags
      });
      spinner.succeed();
    } catch (err) {
      spinner.fail();
      throw err;
    }
    output.print("Export finished (".concat(prettyMs__default.default(Date.now() - start), ")"));
  }
};
async function getOutputPath(destination, dataset, prompt, flags) {
  if (destination === "-") {
    return "-";
  }
  const dstPath = path__default$1.default.isAbsolute(destination) ? destination : path__default$1.default.resolve(process.cwd(), destination);
  let dstStats = await fs__default$1.default.stat(dstPath).catch(noop);
  const looksLikeFile = dstStats ? dstStats.isFile() : path__default$1.default.basename(dstPath).indexOf(".") !== -1;
  if (!dstStats) {
    const createPath = looksLikeFile ? path__default$1.default.dirname(dstPath) : dstPath;
    await fs__default$1.default.mkdir(createPath, {
      recursive: true
    });
  }
  const finalPath = looksLikeFile ? dstPath : path__default$1.default.join(dstPath, "".concat(dataset, ".tar.gz"));
  dstStats = await fs__default$1.default.stat(finalPath).catch(noop);
  if (!flags.overwrite && dstStats && dstStats.isFile()) {
    const shouldOverwrite = await prompt.single({
      type: "confirm",
      message: 'File "'.concat(finalPath, '" already exists, would you like to overwrite it?'),
      default: false
    });
    if (!shouldOverwrite) {
      return false;
    }
  }
  return finalPath;
}
const yellow = str => "\x1B[33m".concat(str, "\x1B[39m");
const helpText$k = '\nOptions\n  --missing On duplicate document IDs, skip importing document in question\n  --replace On duplicate document IDs, replace existing document with imported document\n  --allow-failing-assets Skip assets that cannot be fetched/uploaded\n  --replace-assets Skip reuse of existing assets\n  --skip-cross-dataset-references Skips references to other datasets\n\nRarely used options (should generally not be used)\n  --allow-assets-in-different-dataset Allow asset documents to reference different project/dataset\n  --allow-system-documents Allow system documents like dataset permissions and custom retention to be imported\n\nExamples\n  # Import "moviedb.ndjson" from the current directory to the dataset called "moviedb"\n  sanity dataset import moviedb.ndjson moviedb\n\n  # Import "moviedb.tar.gz" from the current directory to the dataset called "moviedb",\n  # replacing any documents encountered that have the same document IDs\n  sanity dataset import moviedb.tar.gz moviedb --replace\n\n  # Import from a folder containing an ndjson file, such as an extracted tarball\n  # retrieved through "sanity dataset export".\n  sanity dataset import ~/some/folder moviedb\n\n  # Import from a remote URL. Will download and extract the tarball to a temporary\n  # location before importing it.\n  sanity dataset import https://some.url/moviedb.tar.gz moviedb --replace\n';
function toBoolIfSet(flag) {
  return typeof flag === "undefined" ? void 0 : Boolean(flag);
}
function parseFlags(rawFlags) {
  const allowAssetsInDifferentDataset = toBoolIfSet(rawFlags["allow-assets-in-different-dataset"]);
  const allowFailingAssets = toBoolIfSet(rawFlags["allow-failing-assets"]);
  const assetConcurrency = toBoolIfSet(rawFlags["asset-concurrency"]);
  const replaceAssets = toBoolIfSet(rawFlags["replace-assets"]);
  const skipCrossDatasetReferences = toBoolIfSet(rawFlags["skip-cross-dataset-references"]);
  const allowSystemDocuments = toBoolIfSet(rawFlags["allow-system-documents"]);
  const replace = toBoolIfSet(rawFlags.replace);
  const missing = toBoolIfSet(rawFlags.missing);
  return {
    allowAssetsInDifferentDataset,
    allowFailingAssets,
    assetConcurrency,
    skipCrossDatasetReferences,
    allowSystemDocuments,
    replaceAssets,
    replace,
    missing
  };
}
const importDatasetCommand = {
  name: "import",
  group: "dataset",
  signature: "[FILE | FOLDER | URL] [TARGET_DATASET]",
  description: "Import documents to given dataset from either an ndjson file or a gzipped tarball",
  helpText: helpText$k,
  // eslint-disable-next-line max-statements
  action: async (args, context) => {
    const {
      apiClient,
      output,
      chalk,
      fromInitCommand
    } = context;
    const flags = parseFlags(args.extOptions);
    const {
      allowAssetsInDifferentDataset,
      allowFailingAssets,
      assetConcurrency,
      skipCrossDatasetReferences,
      allowSystemDocuments,
      replaceAssets
    } = flags;
    const operation = getMutationOperation(args.extOptions);
    const client = apiClient();
    const [file, target] = args.argsWithoutOptions;
    if (!file) {
      throw new Error('Source file name and target dataset must be specified ("sanity dataset import '.concat(chalk.bold("[file]"), ' [dataset]")'));
    }
    const targetDataset = await determineTargetDataset(target, context);
    debug$2('Target dataset has been set to "'.concat(targetDataset, '"'));
    const isUrl = /^https?:\/\//i.test(file);
    let inputStream;
    let assetsBase;
    let sourceIsFolder = false;
    if (isUrl) {
      debug$2("Input is a URL, streaming from source URL");
      inputStream = await getUrlStream(file);
    } else {
      const sourceFile = path__default$1.default.resolve(process.cwd(), file);
      const fileStats = await fs__default$1.default.stat(sourceFile).catch(() => null);
      if (!fileStats) {
        throw new Error("".concat(sourceFile, " does not exist or is not readable"));
      }
      sourceIsFolder = fileStats.isDirectory();
      if (sourceIsFolder) {
        inputStream = sourceFile;
      } else {
        assetsBase = path__default$1.default.dirname(sourceFile);
        inputStream = await fs$1.createReadStream(sourceFile);
      }
    }
    const importClient = client.clone().config({
      dataset: targetDataset
    });
    const {
      projectId,
      dataset
    } = importClient.config();
    output.print("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E");
    output.print("\u2502                                               \u2502");
    output.print("\u2502 Importing to:                                 \u2502");
    output.print("\u2502 ".concat(chalk.bold("projectId"), ": ").concat(chalk.cyan(projectId).padEnd(44), " \u2502"));
    output.print("\u2502 ".concat(chalk.bold("dataset"), ": ").concat(chalk.cyan(dataset).padEnd(46), " \u2502"));
    output.print("\u2502                                               \u2502");
    output.print("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F");
    output.print("");
    let currentStep;
    let currentProgress;
    let stepStart;
    let spinInterval = null;
    let percent;
    function onProgress(opts) {
      const lengthComputable = opts.total;
      const sameStep = opts.step == currentStep;
      percent = getPercentage(opts);
      if (lengthComputable && opts.total === opts.current) {
        if (spinInterval) {
          clearInterval(spinInterval);
        }
        spinInterval = null;
      }
      if (sameStep) {
        return;
      }
      const prevStep = currentStep;
      const prevStepStart = stepStart || Date.now();
      stepStart = Date.now();
      currentStep = opts.step;
      if (currentProgress && currentProgress.succeed) {
        const timeSpent = prettyMs__default.default(Date.now() - prevStepStart, {
          secondsDecimalDigits: 2
        });
        currentProgress.text = "[100%] ".concat(prevStep, " (").concat(timeSpent, ")");
        currentProgress.succeed();
      }
      currentProgress = output.spinner("[0%] ".concat(opts.step, " (0.00s)")).start();
      if (spinInterval) {
        clearInterval(spinInterval);
        spinInterval = null;
      }
      spinInterval = setInterval(() => {
        const timeSpent = prettyMs__default.default(Date.now() - prevStepStart, {
          secondsDecimalDigits: 2
        });
        if (currentProgress) {
          currentProgress.text = "".concat(percent).concat(opts.step, " (").concat(timeSpent, ")");
        }
      }, 60);
    }
    function endTask(_ref) {
      let {
        success
      } = _ref;
      if (spinInterval) {
        clearInterval(spinInterval);
      }
      spinInterval = null;
      if (success && stepStart && currentProgress) {
        const timeSpent = prettyMs__default.default(Date.now() - stepStart, {
          secondsDecimalDigits: 2
        });
        currentProgress.text = "[100%] ".concat(currentStep, " (").concat(timeSpent, ")");
        currentProgress.succeed();
      } else if (currentProgress) {
        currentProgress.fail();
      }
    }
    try {
      const {
        numDocs,
        warnings
      } = await sanityImport__default.default(inputStream, {
        client: importClient,
        assetsBase,
        operation,
        onProgress,
        allowFailingAssets,
        allowAssetsInDifferentDataset,
        skipCrossDatasetReferences,
        allowSystemDocuments,
        assetConcurrency,
        replaceAssets
      });
      endTask({
        success: true
      });
      output.print('Done! Imported %d documents to dataset "%s"\n', numDocs, targetDataset);
      printWarnings(warnings, output);
    } catch (err) {
      endTask({
        success: false
      });
      const isNonRefConflict = !fromInitCommand && err.response && err.response.statusCode === 409 && err.step !== "strengthen-references";
      if (!isNonRefConflict) {
        throw err;
      }
      const message = [err.message, "", "You probably want either:", " --replace (replace existing documents with same IDs)", " --missing (only import documents that do not already exist)", ""].join("\n");
      const error = new Error(message);
      error.details = err.details;
      error.response = err.response;
      error.responseBody = err.responseBody;
      throw error;
    }
  }
};
async function determineTargetDataset(target, context) {
  const {
    apiClient,
    output,
    prompt
  } = context;
  const client = apiClient();
  if (target) {
    const dsError = validateDatasetName(target);
    if (dsError) {
      throw new Error(dsError);
    }
  }
  debug$2("Fetching available datasets");
  const spinner = output.spinner("Fetching available datasets").start();
  const datasets = await client.datasets.list();
  spinner.succeed("[100%] Fetching available datasets");
  let targetDataset = target ? "".concat(target) : null;
  if (!targetDataset) {
    targetDataset = await chooseDatasetPrompt(context, {
      message: "Select target dataset",
      allowCreation: true
    });
  } else if (!datasets.find(dataset => dataset.name === targetDataset)) {
    debug$2("Target dataset does not exist, prompting for creation");
    const shouldCreate = await prompt.single({
      type: "confirm",
      message: 'Dataset "'.concat(targetDataset, '" does not exist, would you like to create it?'),
      default: true
    });
    if (!shouldCreate) {
      throw new Error('Dataset "'.concat(targetDataset, '" does not exist'));
    }
    await client.datasets.create(targetDataset);
  }
  return targetDataset;
}
function getMutationOperation(flags) {
  const {
    replace,
    missing
  } = flags;
  if (replace && missing) {
    throw new Error("Cannot use both --replace and --missing");
  }
  if (flags.replace) {
    return "createOrReplace";
  }
  if (flags.missing) {
    return "createIfNotExists";
  }
  return "create";
}
function getPercentage(opts) {
  if (!opts.total || typeof opts.current === "undefined") {
    return "";
  }
  const percent = Math.floor(opts.current / opts.total * 100);
  return "[".concat(padStart__default.default("".concat(percent), 3, " "), "%] ");
}
function getUrlStream(url) {
  const request = getIt.getIt([middleware.promise({
    onlyBody: true
  })]);
  return request({
    url,
    stream: true
  });
}
function printWarnings(warnings, output) {
  const assetFails = warnings.filter(warn2 => warn2.type === "asset");
  if (!assetFails.length) {
    return;
  }
  const warn = (output.warn || output.print).bind(output);
  warn(yellow("\u26A0 Failed to import the following %s:"), assetFails.length > 1 ? "assets" : "asset");
  warnings.forEach(warning => {
    warn("  ".concat(warning.url));
  });
}
const listAliasesHandler = async (args, context) => {
  const {
    apiClient,
    output
  } = context;
  const client = apiClient();
  const aliases = await listAliases(client);
  output.print(aliases.map(set => "".concat(ALIAS_PREFIX).concat(set.name, " -> ").concat(set.datasetName || "<unlinked>")).join("\n"));
};
const listDatasetsCommand = {
  name: "list",
  group: "dataset",
  helpText: "",
  signature: "",
  description: "List datasets of your project",
  action: async (args, context) => {
    const {
      apiClient,
      output
    } = context;
    const client = apiClient();
    const datasets = await client.datasets.list();
    output.print(datasets.map(set => set.name).join("\n"));
    await listAliasesHandler(args, context);
  }
};
const helpText$j = "\nOptions\n  --source-maps Enable source maps for built bundles (increases size of bundle)\n  --no-minify Skip minifying built JavaScript (speeds up build, increases size of bundle)\n  --no-build Don't build the studio prior to deploy, instead deploying the version currently in `dist/`\n\nExamples\n  sanity deploy\n  sanity deploy --no-minify --source-maps\n";
const deployCommand = {
  name: "deploy",
  signature: "[SOURCE_DIR] [--no-build]  [--source-maps] [--no-minify]",
  description: "Builds and deploys Sanity Studio to Sanity hosting",
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./deployAction-CEKPux0n.js');
    });
    return mod.default(args, context);
  },
  helpText: helpText$j
};
const helpText$i = "\nExamples\n  sanity undeploy\n";
const undeployCommand = {
  name: "undeploy",
  signature: "",
  description: "Removes the deployed Sanity Studio from Sanity hosting",
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./undeployAction-D1GDUEXq.js');
    });
    return mod.default(args, context);
  },
  helpText: helpText$i
};
const helpText$h = '\nNotes\n  Changing the hostname or port number might require a new entry to the CORS-origins allow list.\n\nOptions\n  --port <port> TCP port to start server on. [default: 3333]\n  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]\n\nExamples\n  sanity dev --host=0.0.0.0\n  sanity dev --port=1942\n';
const devCommand = {
  name: "dev",
  signature: "[--port <port>] [--host <host>]",
  description: "Starts a local dev server for Sanity Studio with live reloading",
  action: async (args, context) => {
    const devAction = await getDevAction();
    return devAction(args, context);
  },
  helpText: helpText$h
};
async function getDevAction() {
  const mod = await Promise.resolve().then(function () {
    return require('./devAction-DaW6697o.js');
  });
  return mod.default;
}
const helpText$g = '\nOptions\n  --replace On duplicate document IDs, replace existing document with specified document(s)\n  --missing On duplicate document IDs, don\'t modify the target document(s)\n  --watch   Write the documents whenever the target file or buffer changes\n  --json5   Use JSON5 file type to allow a "simplified" version of JSON\n  --id <id> Specify a document ID to use. Will fetch remote document ID and populate editor.\n  --dataset NAME to override dataset\n\nExamples\n  # Create the document specified in "myDocument.json".\n  sanity documents create myDocument.json\n\n  # Open configured $EDITOR and create the specified document(s)\n  sanity documents create\n\n  # Fetch document with the ID "myDocId" and open configured $EDITOR with the\n  # current document content (if any). Replace document with the edited version\n  # when the editor closes\n  sanity documents create --id myDocId --replace\n\n  # Open configured $EDITOR and replace the document with the given content\n  # on each save. Use JSON5 file extension and parser for simplified syntax.\n  sanity documents create --id myDocId --watch --replace --json5\n';
const createDocumentsCommand = {
  name: "create",
  group: "documents",
  signature: "[FILE]",
  helpText: helpText$g,
  description: "Create one or more documents",
  // eslint-disable-next-line complexity
  action: async (args, context) => {
    const {
      apiClient,
      output
    } = context;
    const {
      replace,
      missing,
      watch,
      id,
      dataset
    } = args.extOptions;
    const [file] = args.argsWithoutOptions;
    const useJson5 = args.extOptions.json5;
    const client = dataset ? apiClient().clone().config({
      dataset
    }) : apiClient();
    if (replace && missing) {
      throw new Error("Cannot use both --replace and --missing");
    }
    if (id && file) {
      throw new Error("Cannot use --id when specifying a file path");
    }
    let operation = "create";
    if (replace || missing) {
      operation = replace ? "createOrReplace" : "createIfNotExists";
    }
    if (file) {
      const contentPath = path__default$1.default.resolve(process.cwd(), file);
      const content = json5__default.default.parse(await fs__default$1.default.readFile(contentPath, "utf8"));
      const result = await writeDocuments(content, operation, client);
      output.print(getResultMessage(result, operation));
      return;
    }
    const docId = id || uuid.uuid();
    const ext = useJson5 ? "json5" : "json";
    const tmpFile = path__default$1.default.join(os__default.default.tmpdir(), "sanity-cli", "".concat(docId, ".").concat(ext));
    const stringify = useJson5 ? json5__default.default.stringify : JSON.stringify;
    const defaultValue = id && (await client.getDocument(id)) || {
      _id: docId,
      _type: "specify-me"
    };
    await fs__default$1.default.mkdir(path__default$1.default.join(os__default.default.tmpdir(), "sanity-cli"), {
      recursive: true
    });
    await fs__default$1.default.writeFile(tmpFile, stringify(defaultValue, null, 2), "utf8");
    const editor = getEditor();
    if (watch) {
      registerUnlinkOnSigInt(tmpFile);
      output.print("Watch mode: ".concat(tmpFile));
      output.print("Watch mode: Will write documents on each save.");
      output.print("Watch mode: Press Ctrl + C to cancel watch mode.");
      chokidar__default.default.watch(tmpFile).on("change", () => {
        output.print("");
        return readAndPerformCreatesFromFile(tmpFile);
      });
      execa__default.default(editor.bin, editor.args.concat(tmpFile), {
        stdio: "inherit"
      });
    } else {
      execa__default.default.sync(editor.bin, editor.args.concat(tmpFile), {
        stdio: "inherit"
      });
      await readAndPerformCreatesFromFile(tmpFile);
      await fs__default$1.default.unlink(tmpFile).catch(noop__default.default);
    }
    async function readAndPerformCreatesFromFile(filePath) {
      let content;
      try {
        content = json5__default.default.parse(await fs__default$1.default.readFile(filePath, "utf8"));
      } catch (err) {
        output.error("Failed to read input: ".concat(err.message));
        return;
      }
      if (isEqual__default.default(content, defaultValue)) {
        output.print("Value not modified, doing nothing.");
        output.print("Modify document to trigger creation.");
        return;
      }
      try {
        const writeResult = await writeDocuments(content, operation, client);
        output.print(getResultMessage(writeResult, operation));
      } catch (err) {
        output.error("Failed to write documents: ".concat(err.message));
        if (err.message.includes("already exists")) {
          output.error("Perhaps you want to use `--replace` or `--missing`?");
        }
      }
    }
  }
};
function registerUnlinkOnSigInt(tmpFile) {
  process.on("SIGINT", async () => {
    await fs__default$1.default.unlink(tmpFile).catch(noop__default.default);
    process.exit(130);
  });
}
function writeDocuments(documents, operation, client) {
  const docs = Array.isArray(documents) ? documents : [documents];
  if (docs.length === 0) {
    throw new Error("No documents provided");
  }
  const mutations = docs.map((doc, index) => {
    validateDocument(doc, index, docs);
    if (operation === "create") {
      return {
        create: doc
      };
    }
    if (operation === "createIfNotExists") {
      if (isIdentifiedSanityDocument(doc)) {
        return {
          createIfNotExists: doc
        };
      }
      throw new Error("Missing required _id attribute for ".concat(operation));
    }
    if (operation === "createOrReplace") {
      if (isIdentifiedSanityDocument(doc)) {
        return {
          createOrReplace: doc
        };
      }
      throw new Error("Missing required _id attribute for ".concat(operation));
    }
    throw new Error("Unsupported operation ".concat(operation));
  });
  return client.transaction(mutations).commit();
}
function validateDocument(doc, index, arr) {
  const isSingle = arr.length === 1;
  if (!isPlainObject__default.default(doc)) {
    throw new Error(getErrorMessage("must be an object", index, isSingle));
  }
  if (!isSanityDocumentish(doc)) {
    throw new Error(getErrorMessage("must have a `_type` property of type string", index, isSingle));
  }
}
function isSanityDocumentish(doc) {
  return doc !== null && typeof doc === "object" && "_type" in doc && typeof doc._type === "string";
}
function isIdentifiedSanityDocument(doc) {
  return isSanityDocumentish(doc) && "_id" in doc;
}
function getErrorMessage(message, index, isSingle) {
  return isSingle ? "Document ".concat(message) : "Document at index ".concat(index, " ").concat(message);
}
function getResultMessage(result, operation) {
  const joiner = "\n  - ";
  if (operation === "createOrReplace") {
    return "Upserted:\n  - ".concat(result.results.map(res => res.id).join(joiner));
  }
  if (operation === "create") {
    return "Created:\n  - ".concat(result.results.map(res => res.id).join(joiner));
  }
  const created = [];
  const skipped = [];
  for (const res of result.results) {
    if (res.operation === "update") {
      skipped.push(res.id);
    } else {
      created.push(res.id);
    }
  }
  if (created.length > 0 && skipped.length > 0) {
    return ["Created:\n  - ".concat(created.join(joiner)), "Skipped (already exists):".concat(joiner).concat(skipped.join(joiner))].join("\n\n");
  } else if (created.length > 0) {
    return "Created:\n  - ".concat(created.join(joiner));
  }
  return "Skipped (already exists):\n  - ".concat(skipped.join(joiner));
}
function getEditor() {
  const defaultEditor = /^win/.test(process.platform) ? "notepad" : "vim";
  const editor = process.env.VISUAL || process.env.EDITOR || defaultEditor;
  const args = editor.split(/\s+/);
  const bin = args.shift() || "";
  return {
    bin,
    args
  };
}
const helpText$f = '\nDelete a document from the projects configured dataset\n\nOptions\n  --dataset NAME to override dataset\n\nExample\n  # Delete the document with the ID "myDocId"\n  sanity documents delete myDocId\n\n  # ID wrapped in double or single quote works equally well\n  sanity documents delete \'myDocId\'\n\n  # Delete document with ID "someDocId" from dataset "blog"\n  sanity documents delete --dataset=blog someDocId\n\n  # Delete the document with ID "doc1" and "doc2"\n  sanity documents delete doc1 doc2\n';
const deleteDocumentsCommand = {
  name: "delete",
  group: "documents",
  signature: "[ID] [...IDS]",
  helpText: helpText$f,
  description: "Delete a document by ID",
  action: async (args, context) => {
    const {
      apiClient,
      output,
      chalk
    } = context;
    const {
      dataset
    } = args.extOptions;
    const ids = args.argsWithoutOptions.map(str => "".concat(str));
    if (!ids.length) {
      throw new Error("Document ID must be specified");
    }
    const client = dataset ? apiClient().clone().config({
      dataset
    }) : apiClient();
    const transaction = ids.reduce((trx, id) => trx.delete(id), client.transaction());
    try {
      const {
        results
      } = await transaction.commit();
      const deleted = results.filter(res => res.operation === "delete").map(res => res.id);
      const notFound = ids.filter(id => !deleted.includes(id));
      if (deleted.length > 0) {
        output.print("Deleted ".concat(deleted.length, " ").concat(pluralize__default.default("document", deleted.length)));
      }
      if (notFound.length > 0) {
        output.error(chalk.red("".concat(pluralize__default.default("Document", notFound.length), " not found: ").concat(notFound.join(", "))));
      }
    } catch (err) {
      throw new Error("Failed to delete ".concat(pluralize__default.default("document", ids.length), ":\n").concat(err.message));
    }
  }
};
const documentsGroup = {
  name: "documents",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Manages documents in your Sanity Content Lake datasets"
};
const identity = inp => inp;
function colorizeJson(input, chalk) {
  const formatters = {
    punctuator: chalk.white,
    key: chalk.white,
    string: chalk.green,
    number: chalk.yellow,
    literal: chalk.bold,
    whitespace: identity
  };
  const json = JSON.stringify(input, null, 2);
  return tokenize__default.default(json).map((token, i, arr) => {
    const prevToken = i === 0 ? token : arr[i - 1];
    if (token.type === "string" && prevToken.type === "whitespace" && /^\n\s+$/.test(prevToken.value)) {
      return {
        ...token,
        type: "key"
      };
    }
    return token;
  }).map(token => {
    const formatter = formatters[token.type] || identity;
    return formatter(token.raw);
  }).join("");
}
const helpText$e = "\nGet and print a document from the projects configured dataset\n\nOptions\n  --pretty colorized JSON output\n  --dataset NAME to override dataset\n\nExamples\n  # Get the document with the ID \"myDocId\"\n  sanity documents get myDocId\n\n  # ID wrapped in double or single quote works equally well\n  sanity documents get 'myDocId'\n";
const getDocumentsCommand = {
  name: "get",
  group: "documents",
  signature: "[DOCUMENT_ID]",
  helpText: helpText$e,
  description: "Get and print a document by ID",
  action: async (args, context) => {
    const {
      apiClient,
      output,
      chalk
    } = context;
    const {
      pretty,
      dataset
    } = args.extOptions;
    const [docId] = args.argsWithoutOptions.map(str => "".concat(str));
    if (!docId) {
      throw new Error("Document ID must be specified");
    }
    const client = dataset ? apiClient().clone().config({
      dataset
    }) : apiClient();
    try {
      const doc = await client.getDocument(docId);
      if (!doc) {
        throw new Error("Document ".concat(docId, " not found"));
      }
      output.print(pretty ? colorizeJson(doc, chalk) : JSON.stringify(doc, null, 2));
    } catch (err) {
      throw new Error("Failed to fetch document:\n".concat(err.message));
    }
  }
};
const defaultApiVersion = "v2022-06-01";
const helpText$d = "\nRun a query against the projects configured dataset\n\nOptions\n  --pretty colorized JSON output\n  --dataset NAME to override dataset\n  --project PROJECT to override project ID\n  --anonymous Send the query without any authorization token\n  --api-version API version to use (defaults to `".concat(defaultApiVersion, '`)\n\nEnvironment variables\n  `SANITY_CLI_QUERY_API_VERSION` - will use the defined API version,\n  unless `--api-version` is specified.\n\nExamples\n  # Fetch 5 documents of type "movie"\n  sanity documents query \'*[_type == "movie"][0..4]\'\n\n  # Fetch title of the oldest movie in the dataset named "staging"\n  sanity documents query \'*[_type == "movie"]|order(releaseDate asc)[0]{title}\' --dataset staging\n\n  # Use API version v2021-06-07 and do a query\n  sanity documents query --api-version v2021-06-07 \'*[_id == "header"] { "headerText": pt::text(body) }\'\n');
var queryDocumentsCommand = {
  name: "query",
  group: "documents",
  signature: "[QUERY]",
  helpText: helpText$d,
  description: "Query for documents",
  action: async (args, context) => {
    var _a, _b;
    const {
      pretty,
      dataset,
      project,
      anonymous,
      "api-version": apiVersion
    } = await parseCliFlags$1(args);
    const {
      apiClient,
      output,
      chalk,
      cliConfig
    } = context;
    const [query] = args.argsWithoutOptions;
    if (!query) {
      throw new Error("Query must be specified");
    }
    if (!apiVersion) {
      output.warn(chalk.yellow("--api-version not specified, using `".concat(defaultApiVersion, "`")));
    }
    const requireDataset = !dataset;
    const requireProject = !project;
    const requireUser = !anonymous;
    if (requireProject && !((_a = cliConfig == null ? void 0 : cliConfig.api) == null ? void 0 : _a.projectId)) {
      throw new Error("No project configured in CLI config - either configure one, or use `--project` flag");
    }
    if (requireDataset && !((_b = cliConfig == null ? void 0 : cliConfig.api) == null ? void 0 : _b.dataset)) {
      throw new Error("No dataset configured in CLI config - either configure one, or use `--dataset` flag");
    }
    const baseClient = apiClient({
      requireProject,
      requireUser
    }).clone();
    const {
      dataset: originalDataset,
      projectId: originalProjectId
    } = baseClient.config();
    const client = baseClient.config({
      projectId: project || originalProjectId,
      dataset: dataset || originalDataset,
      apiVersion: apiVersion || defaultApiVersion
    });
    try {
      const docs = await client.fetch(query);
      if (!docs) {
        throw new Error("Query returned no results");
      }
      output.print(pretty ? colorizeJson(docs, chalk) : JSON.stringify(docs, null, 2));
    } catch (err) {
      throw new Error("Failed to run query:\n".concat(err.message));
    }
  }
};
function parseCliFlags$1(args) {
  const fallbackApiVersion = process.env.SANITY_CLI_QUERY_API_VERSION;
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("pretty", {
    type: "boolean",
    default: false
  }).option("dataset", {
    type: "string"
  }).option("project", {
    type: "string"
  }).option("anonymous", {
    type: "boolean",
    default: false
  }).option("api-version", {
    type: "string",
    default: fallbackApiVersion
  }).argv;
}
const description$1 = "Downloads and validates all document specified in a workspace";
const helpText$c = "\nOptions\n  -y, --yes Skips the first confirmation prompt.\n  --workspace <name> The name of the workspace to use when downloading and validating all documents.\n  --dataset <name> Override the dataset used. By default, this is derived from the given workspace.\n  --file <filepath> Provide a path to either an .ndjson file or a tarball containing an .ndjson file.\n  --format <pretty|ndjson|json> The output format used to print the found validation markers and report progress.\n  --level <error|warning|info> The minimum level reported out. Defaults to warning.\n  --max-custom-validation-concurrency <number> Specify how many custom validators can run concurrently. Defaults to 5.\n\nExamples\n  # Validates all documents in a Sanity project with more than one workspace\n  sanity documents validate --workspace default\n\n  # Override the dataset specified in the workspace\n  sanity documents validate --workspace default --dataset staging\n\n  # Save the results of the report into a file\n  sanity documents validate > report.txt\n\n  # Report out info level validation markers too\n  sanity documents validate --level info\n";
const validateDocumentsCommand$1 = {
  name: "validate",
  group: "documents",
  signature: "",
  description: description$1,
  helpText: helpText$c,
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./validateAction-CEWLllLc.js');
    });
    return mod.default(args, context);
  }
};
const helpText$b = "\nOptions\n  --with-user-token Prime access token from CLI config into getCliClient()\n  --mock-browser-env Mocks a browser-like environment using jsdom\n\nExamples\n  # Run the script at some/script.js in Sanity context\n  sanity exec some/script.js\n\n  # Run the script at migrations/fullname.ts and configure `getCliClient()`\n  # from `sanity/cli`to include the current user's token\n  sanity exec migrations/fullname.ts --with-user-token\n\n  # Run the script at scripts/browserScript.js in a mock browser environment\n  sanity exec scripts/browserScript.js --mock-browser-env\n\n  # Pass arbitrary arguments to scripts by separating them with a `--`.\n  # Arguments are available in `process.argv` as they would in regular node scripts\n  # eg the following command would yield a `process.argv` of:\n  # ['/path/to/node', '/path/to/myscript.js', '--dry-run', 'positional-argument']\n  sanity exec --mock-browser-env myscript.js -- --dry-run positional-argument\n";
const execCommand = {
  name: "exec",
  signature: "SCRIPT",
  description: "Executes a script within the Sanity Studio context",
  helpText: helpText$b,
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./execScript-B-FaGea5.js');
    });
    return mod.default(args, context);
  }
};
const helpText$a = "\nOptions\n  --api <api-id> Undeploy API with this ID (project, dataset and tag flags takes preference)\n  --project <projectId> Project ID to delete GraphQL API for\n  --dataset <dataset> Delete GraphQL API for the given dataset\n  --tag <tag> Delete GraphQL API for the given tag (defaults to 'default')\n  --force Skip confirmation prompt, forcefully undeploying the GraphQL API\n\nExamples\n  sanity graphql undeploy\n  sanity graphql undeploy --api ios\n  sanity graphql undeploy --dataset staging\n  sanity graphql undeploy --dataset staging --tag next\n";
const deleteGraphQLAPICommand = {
  name: "undeploy",
  group: "graphql",
  signature: "",
  description: "Remove a deployed GraphQL API",
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./deleteApiAction-CZ0DEnBt.js');
    });
    return mod.default(args, context);
  },
  helpText: helpText$a
};
const helpText$9 = "\nOptions\n  --dry-run Validate defined APIs, exiting with an error on breaking changes\n  --force Deploy API without confirming breaking changes\n  --api <api-id> Only deploy API with this ID. Can be specified multiple times.\n\nThe following options will override any setting from the CLI configuration file\n(sanity.cli.js/sanity.cli.ts) - and applies to ALL defined APIs defined in that\nconfiguration file. Tread with caution!\n\n  --tag Deploy API(s) to given tag (defaults to 'default')\n  --dataset <name> Deploy API for the given dataset\n  --generation <gen1|gen2|gen3> API generation to deploy (defaults to 'gen3')\n  --non-null-document-fields Use non-null document fields (_id, _type etc)\n  --playground Enable GraphQL playground for easier debugging\n  --no-playground Disable GraphQL playground\n  --with-union-cache *Experimental:* Enable union cache that optimizes schema generation for schemas with many self referencing types\n\nExamples\n  # Deploy all defined GraphQL APIs\n  sanity graphql deploy\n\n  # Validate defined GraphQL APIs, check for breaking changes, skip deploy\n  sanity graphql deploy --dry-run\n\n  # Deploy only the GraphQL APIs with the IDs \"staging\" and \"ios\"\n  sanity graphql deploy --api staging --api ios\n\n  # Deploy all defined GraphQL APIs, overriding any playground setting\n  sanity graphql deploy --playground\n";
const deployGraphQLAPICommand = {
  name: "deploy",
  signature: "",
  group: "graphql",
  description: "Deploy a GraphQL API from the current Sanity schema",
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./deployApiAction-DM7ggQX1.js');
    });
    return mod.default(args, context);
  },
  helpText: helpText$9
};
const graphqlGroup = {
  name: "graphql",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Deploys changes to your project's GraphQL API(s)"
};
const helpText$8 = "\nExamples\n  sanity graphql list\n";
const listGraphQLAPIsCommand = {
  name: "list",
  signature: "",
  group: "graphql",
  description: "Lists all the GraphQL endpoints deployed for this project",
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./listApisAction-DApqZpgc.js');
    });
    return mod.default(args, context);
  },
  helpText: helpText$8
};
const createHookCommand = {
  name: "create",
  group: "hook",
  signature: "",
  helpText: "",
  description: "Create a new hook for the given dataset",
  action: async (args, context) => {
    const {
      apiClient,
      output
    } = context;
    const client = apiClient();
    const {
      projectId
    } = client.config();
    if (!projectId) {
      throw new Error("No project ID found");
    }
    const projectInfo = (await client.projects.getById(projectId)) || {};
    const organizationId = projectInfo.organizationId || "personal";
    const manageUrl = "https://www.sanity.io/organizations/".concat(organizationId, "/project/").concat(projectId, "/api/webhooks/new");
    output.print("Opening ".concat(manageUrl));
    open__default.default(manageUrl);
  }
};
const deleteHookCommand = {
  name: "delete",
  group: "hook",
  signature: "[NAME]",
  helpText: "",
  description: "Delete a hook within your project",
  action: async (args, context) => {
    const {
      apiClient
    } = context;
    const [name] = args.argsWithoutOptions;
    const client = apiClient();
    const hookId = await promptForHook$1(name, context);
    try {
      await client.clone().config({
        apiVersion: "2021-10-04"
      }).request({
        method: "DELETE",
        uri: "/hooks/".concat(hookId)
      });
    } catch (err) {
      throw new Error("Hook deletion failed:\n".concat(err.message));
    }
  }
};
async function promptForHook$1(specified, context) {
  const specifiedName = specified && specified.toLowerCase();
  const {
    prompt,
    apiClient
  } = context;
  const client = apiClient();
  const hooks = await client.clone().config({
    apiVersion: "2021-10-04"
  }).request({
    uri: "/hooks",
    json: true
  });
  if (specifiedName) {
    const selected = hooks.filter(hook => hook.name.toLowerCase() === specifiedName)[0];
    if (!selected) {
      throw new Error('Hook with name "'.concat(specified, ' not found"'));
    }
    return selected.id;
  }
  const choices = hooks.map(hook => ({
    value: hook.id,
    name: hook.name
  }));
  return prompt.single({
    message: "Select hook to delete",
    type: "list",
    choices
  });
}
const hookGroup = {
  name: "hook",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Sets up and manages webhooks within your Sanity project"
};
const printHookAttemptCommand = {
  name: "attempt",
  group: "hook",
  signature: "ATTEMPT_ID",
  helpText: "",
  description: "Print details of a given webhook delivery attempt",
  action: async (args, context) => {
    const {
      apiClient,
      output
    } = context;
    const [attemptId] = args.argsWithoutOptions;
    const client = apiClient();
    let attempt;
    try {
      attempt = await client.request({
        uri: "/hooks/attempts/".concat(attemptId)
      });
    } catch (err) {
      throw new Error("Hook attempt retrieval failed:\n".concat(err.message));
    }
    const {
      createdAt,
      resultCode,
      resultBody,
      failureReason,
      inProgress
    } = attempt;
    output.print("Date: ".concat(createdAt));
    output.print("Status: ".concat(getStatus(attempt)));
    output.print("Status code: ".concat(resultCode));
    if (attempt.isFailure) {
      output.print("Failure: ".concat(formatFailure(attempt)));
    }
    if (!inProgress && (!failureReason || failureReason === "http")) {
      const body = resultBody ? "\n---\n".concat(resultBody, "\n---\n") : "<empty>";
      output.print("Response body: ".concat(body));
    }
  }
};
function formatFailure(attempt) {
  let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
  const {
    includeHelp
  } = options;
  const {
    id,
    failureReason,
    resultCode
  } = attempt;
  const help = includeHelp ? "(run `sanity hook attempt ".concat(id, "` for details)") : "";
  switch (failureReason) {
    case "http":
      return "HTTP ".concat(resultCode, " ").concat(help);
    case "timeout":
      return "Request timed out";
    case "network":
      return "Network error";
  }
  return "Unknown error";
}
function getStatus(attempt) {
  if (attempt.isFailure) {
    return "Failed";
  }
  if (attempt.inProgress) {
    return "In progress";
  }
  return "Delivered";
}
const listHookLogsCommand = {
  name: "logs",
  group: "hook",
  signature: "[NAME]",
  helpText: "",
  description: "List latest log entries for a given hook",
  action: async (args, context) => {
    const {
      apiClient
    } = context;
    const flags = args.extOptions;
    const [name] = args.argsWithoutOptions;
    const client = apiClient();
    const hookId = await promptForHook(name, context);
    let messages;
    let attempts;
    try {
      messages = await client.request({
        uri: "/hooks/".concat(hookId, "/messages")
      });
      attempts = await client.request({
        uri: "/hooks/".concat(hookId, "/attempts")
      });
    } catch (err) {
      throw new Error("Hook logs retrieval failed:\n".concat(err.message));
    }
    const groupedAttempts = groupBy__default.default(attempts, "messageId");
    const populated = messages.map(msg => ({
      ...msg,
      attempts: groupedAttempts[msg.id]
    }));
    const totalMessages = messages.length - 1;
    populated.forEach((message, i) => {
      printMessage(message, context, {
        detailed: flags.detailed
      });
      printSeparator(context, totalMessages === i);
    });
  }
};
async function promptForHook(specified, context) {
  const specifiedName = specified && specified.toLowerCase();
  const {
    prompt,
    apiClient
  } = context;
  const client = apiClient();
  const hooks = await client.clone().config({
    apiVersion: "2021-10-04"
  }).request({
    uri: "/hooks",
    json: true
  });
  if (specifiedName) {
    const selected = hooks.filter(hook => hook.name.toLowerCase() === specifiedName)[0];
    if (!selected) {
      throw new Error('Hook with name "'.concat(specified, ' not found"'));
    }
    return selected.id;
  }
  if (hooks.length === 0) {
    throw new Error("No hooks currently registered");
  }
  if (hooks.length === 1) {
    return hooks[0].id;
  }
  const choices = hooks.map(hook => ({
    value: hook.id,
    name: hook.name
  }));
  return prompt.single({
    message: "Select hook to list logs for",
    type: "list",
    choices
  });
}
function printSeparator(context, skip) {
  if (!skip) {
    context.output.print("---\n");
  }
}
function printMessage(message, context, options) {
  const {
    detailed
  } = options;
  const {
    output,
    chalk
  } = context;
  output.print("Date: ".concat(message.createdAt));
  output.print("Status: ".concat(message.status));
  output.print("Result code: ".concat(message.resultCode));
  if (message.failureCount > 0) {
    output.print("Failures: ".concat(message.failureCount));
  }
  if (detailed) {
    output.print("Payload:");
    output.print(util.inspect(JSON.parse(message.payload), {
      colors: true
    }));
  }
  if (detailed && message.attempts) {
    output.print("Attempts:");
    message.attempts.forEach(attempt => {
      const date = attempt.createdAt.replace(/\.\d+Z$/, "Z");
      const prefix = "  [".concat(date, "]");
      if (attempt.inProgress) {
        output.print("".concat(prefix, " ").concat(chalk.yellow("Pending")));
      } else if (attempt.isFailure) {
        const failure = formatFailure(attempt, {
          includeHelp: true
        });
        output.print("".concat(prefix, " ").concat(chalk.yellow("Failure: ".concat(failure))));
      } else {
        output.print("".concat(prefix, " Success: HTTP ").concat(attempt.resultCode, " (").concat(attempt.duration, "ms)"));
      }
    });
  }
  output.print("");
}
const listHooksCommand = {
  name: "list",
  group: "hook",
  signature: "",
  helpText: "",
  description: "List hooks for a given project",
  action: async (args, context) => {
    const {
      apiClient,
      output
    } = context;
    const client = apiClient();
    let hooks;
    try {
      hooks = await client.clone().config({
        apiVersion: "2021-10-04"
      }).request({
        uri: "/hooks"
      });
    } catch (err) {
      throw new Error("Hook list retrieval failed:\n".concat(err.message));
    }
    hooks.forEach(hook => {
      output.print("Name: ".concat(hook.name));
      output.print("Dataset: ".concat(hook.dataset));
      output.print("URL: ".concat(hook.url));
      if (hook.type === "document") {
        output.print("HTTP method: ".concat(hook.httpMethod));
        if (hook.description) {
          output.print("Description: ".concat(hook.description));
        }
      }
      output.print("");
    });
  }
};
const MIGRATIONS_DIRECTORY = "migrations";
const MIGRATION_SCRIPT_EXTENSIONS = ["mjs", "js", "ts", "cjs"];
const minimalAdvanced = _ref2 => {
  let {
    migrationName,
    documentTypes
  } = _ref2;
  return "import {defineMigration, patch, at, setIfMissing} from 'sanity/migrate'\n\n/**\n * this migration will set `Default title` on all documents that are missing a title\n * and make `true` the default value for the `enabled` field\n */\nexport default defineMigration({\n  title: '".concat(migrationName, "',\n").concat(documentTypes.length > 0 ? "  documentTypes: [".concat(documentTypes.map(t => JSON.stringify(t)).join(", "), "],\n") : "", "\n  async *migrate(documents, context) {\n    for await (const document of documents()) {\n      yield patch(document._id, [\n        at('title', setIfMissing('Default title')),\n        at('enabled', setIfMissing(true)),\n      ])\n    }\n  }\n})\n");
};
const minimalSimple = _ref3 => {
  let {
    migrationName,
    documentTypes
  } = _ref3;
  return "import {at, defineMigration, setIfMissing, unset} from 'sanity/migrate'\n\nexport default defineMigration({\n  title: '".concat(migrationName, "',\n").concat(documentTypes.length > 0 ? "  documentTypes: [".concat(documentTypes.map(t => JSON.stringify(t)).join(", "), "],\n") : "", "\n  migrate: {\n    document(doc, context) {\n      // this will be called for every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n\n      return at('title', setIfMissing('Default title'))\n    },\n    node(node, path, context) {\n      // this will be called for every node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n\n      if (typeof node === 'string' && node === 'deleteme') {\n        return unset()\n      }\n    },\n    object(node, path, context) {\n      // this will be called for every object node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n      if (node._type === 'author') {\n        // make sure all authors objects have a books array\n        return at('books', setIfMissing([]))\n      }\n    },\n    array(node, path, context) {\n      // this will be called for every array node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n    },\n    string(node, path, context) {\n      // this will be called for every string node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n    },\n    number(node, path, context) {\n      // this will be called for every number node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n    },\n    boolean(node, path, context) {\n      // this will be called for every boolean node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n    },\n    null(node, path, context) {\n      // this will be called for every null node in every document of the matching type\n      // any patch returned will be applied to the document\n      // you can also return mutations that touches other documents\n    },\n  },\n})\n");
};
const renameField = _ref4 => {
  let {
    migrationName,
    documentTypes
  } = _ref4;
  return "import {defineMigration, at, setIfMissing, unset} from 'sanity/migrate'\n\nconst from = 'oldFieldName'\nconst to = 'newFieldName'\n\nexport default defineMigration({\n  title: '".concat(migrationName, "',\n").concat(documentTypes.length > 0 ? "  documentTypes: [".concat(documentTypes.map(t => JSON.stringify(t)).join(", "), "],\n") : "", "\n  migrate: {\n    document(doc, context) {\n      return [\n        at(to, setIfMissing(doc[from])),\n        at(from, unset())\n      ]\n    }\n  }\n})\n");
};
const renameType = _ref5 => {
  let {
    migrationName,
    documentTypes
  } = _ref5;
  return "import {defineMigration, at, set} from 'sanity/migrate'\n\nconst oldType = 'old'\nconst newType = 'new'\n\nexport default defineMigration({\n  title: '".concat(migrationName, "',\n").concat(documentTypes.length > 0 ? "  documentTypes: [".concat(documentTypes.map(t => JSON.stringify(t)).join(", "), "],\n") : "", "\n  migrate: {\n    object(object, path, context) {\n      if (object._type === oldType) {\n        return at('_type', set(newType))\n      }\n    }\n  }\n})\n");
};
const stringToPTE = _ref6 => {
  let {
    migrationName,
    documentTypes
  } = _ref6;
  return "import {pathsAreEqual, stringToPath} from 'sanity'\nimport {defineMigration, set} from 'sanity/migrate'\n\nconst targetPath = stringToPath('some.path')\n\nexport default defineMigration({\n  title: '".concat(migrationName, "',\n").concat(documentTypes.length > 0 ? "  documentTypes: [".concat(documentTypes.map(t => JSON.stringify(t)).join(", "), "],\n") : "", "\n  migrate: {\n    string(node, path, ctx) {\n      if (pathsAreEqual(path, targetPath)) {\n        return set([\n          {\n            style: 'normal',\n            _type: 'block',\n            children: [\n              {\n                _type: 'span',\n                marks: [],\n                text: node,\n              },\n            ],\n            markDefs: [],\n          },\n        ])\n      }\n    },\n  },\n})\n");
};
const helpText$7 = '\nExamples:\n  # Create a new migration, prompting for title and options\n  sanity migration create\n\n  # Create a new migration with the provided title, prompting for options\n  sanity migration create "Rename field from location to address"\n';
const TEMPLATES = [{
  name: "Minimalistic migration to get you started",
  template: minimalSimple
}, {
  name: "Rename an object type",
  template: renameType
}, {
  name: "Rename a field",
  template: renameField
}, {
  name: "Convert string field to Portable Text",
  template: stringToPTE
}, {
  name: "Advanced template using async iterators providing more fine grained control",
  template: minimalAdvanced
}];
const createMigrationCommand = {
  name: "create",
  group: "migration",
  signature: "[TITLE]",
  helpText: helpText$7,
  description: "Create a new migration within your project",
  action: async (args, context) => {
    const {
      output,
      prompt,
      workDir,
      chalk
    } = context;
    let [title] = args.argsWithoutOptions;
    while (!(title == null ? void 0 : title.trim())) {
      title = await prompt.single({
        type: "input",
        suffix: ' (e.g. "Rename field from location to address")',
        message: "Title of migration"
      });
      if (!title.trim()) {
        output.error(chalk.red("Name cannot be empty"));
      }
    }
    const types = await prompt.single({
      type: "input",
      suffix: " (optional)",
      message: "Type of documents to migrate. You can add multiple types separated by comma"
    });
    const templatesByName = Object.fromEntries(TEMPLATES.map(t => [t.name, t]));
    const template = await prompt.single({
      type: "list",
      message: "Select a template",
      choices: TEMPLATES.map(definedTemplate => ({
        name: definedTemplate.name,
        value: definedTemplate.name
      }))
    });
    const sluggedName = deburr__default.default(title.toLowerCase()).replace(/\s+/g, "-").replace(/[^a-z0-9-]/g, "");
    const destDir = path__default.default.join(MIGRATIONS_DIRECTORY, sluggedName);
    if (node_fs.existsSync(destDir)) {
      if (!(await prompt.single({
        type: "confirm",
        message: "Migration directory ./".concat(destDir, " already exists. Overwrite?"),
        default: false
      }))) {
        return;
      }
    }
    node_fs.mkdirSync(destDir, {
      recursive: true
    });
    const renderedTemplate = (templatesByName[template].template || minimalSimple)({
      migrationName: title,
      documentTypes: types.split(",").map(t => t.trim()).filter(Boolean)
    });
    const definitionFile = path__default.default.join(destDir, "index.ts");
    await promises$1.writeFile(path__default.default.join(workDir, definitionFile), renderedTemplate);
    output.print();
    output.print("".concat(chalk.green("\u2713"), " Migration created!"));
    output.print();
    output.print("Next steps:");
    output.print("Open ./".concat(chalk.bold(definitionFile), " in your code editor and write the code for your migration."));
    output.print("Dry run the migration with:\n`".concat(chalk.bold("sanity migration run ".concat(sluggedName, " --project=<projectId> --dataset <dataset> ")), "`"));
    output.print("Run the migration against a dataset with:\n `".concat(chalk.bold("sanity migration run ".concat(sluggedName, " --project=<projectId> --dataset <dataset> --no-dry-run")), "`"));
    output.print();
    output.print("\u{1F449} Learn more about schema and content migrations at ".concat(chalk.bold("https://www.sanity.io/docs/schema-and-content-migrations")));
  }
};
function resolveMigrationScript(workDir, migrationName) {
  return [migrationName, path__default.default.join(migrationName, "index")].flatMap(location => MIGRATION_SCRIPT_EXTENSIONS.map(ext => {
    const relativePath = path__default.default.join(MIGRATIONS_DIRECTORY, "".concat(location, ".").concat(ext));
    const absolutePath = path__default.default.resolve(workDir, relativePath);
    let mod;
    try {
      mod = require(absolutePath);
    } catch (err) {
      if (err.code !== "MODULE_NOT_FOUND") {
        throw new Error("Error: ".concat(err.message, '"'));
      }
    }
    return {
      relativePath,
      absolutePath,
      mod
    };
  }));
}
function isLoadableMigrationScript(script) {
  if (typeof script.mod === "undefined" || !isPlainObject__default.default(script.mod.default)) {
    return false;
  }
  const mod = script.mod.default;
  return typeof mod.title === "string" && mod.migrate !== void 0;
}
const helpText$6 = "";
const listMigrationCommand = {
  name: "list",
  group: "migration",
  signature: "",
  helpText: helpText$6,
  description: "List available migrations",
  action: async (_, context) => {
    const {
      workDir,
      output,
      chalk
    } = context;
    try {
      const migrations = await resolveMigrations(workDir);
      if (migrations.length === 0) {
        output.print("No migrations found in migrations folder of the project");
        output.print("\nRun ".concat(chalk.green("`sanity migration create <NAME>`"), " to create a new migration"));
        return;
      }
      const table = new consoleTablePrinter.Table({
        title: "Found ".concat(migrations.length, " migrations in project"),
        columns: [{
          name: "id",
          title: "ID",
          alignment: "left"
        }, {
          name: "title",
          title: "Title",
          alignment: "left"
        }]
      });
      migrations.forEach(definedMigration => {
        table.addRow({
          id: definedMigration.id,
          title: definedMigration.migration.title
        });
      });
      table.printTable();
      output.print("\nRun `sanity migration run <ID>` to run a migration");
    } catch (error) {
      if (error.code === "ENOENT") {
        output.print("No migrations folder found in the project");
        output.print("\nRun ".concat(chalk.green("`sanity migration create <NAME>`"), " to create a new migration"));
        return;
      }
      throw new Error("An error occurred while listing migrations: ".concat(error.message));
    }
  }
};
async function resolveMigrations(workDir) {
  let unregister;
  {
    unregister = node.register({
      target: "node".concat(process.version.slice(1))
    }).unregister;
  }
  const migrationsDir = path__default.default.join(workDir, MIGRATIONS_DIRECTORY);
  const migrationEntries = await promises$1.readdir(migrationsDir, {
    withFileTypes: true
  });
  const migrations = [];
  for (const entry of migrationEntries) {
    const entryName = entry.isDirectory() ? entry.name : removeMigrationScriptExtension(entry.name);
    const candidates = resolveMigrationScript(workDir, entryName).filter(isLoadableMigrationScript);
    for (const candidate of candidates) {
      migrations.push({
        id: entryName,
        migration: candidate.mod.default
      });
    }
  }
  if (unregister) {
    unregister();
  }
  return migrations;
}
function removeMigrationScriptExtension(fileName) {
  return MIGRATION_SCRIPT_EXTENSIONS.reduce((name, ext) => name.endsWith(".".concat(ext)) ? path__default.default.basename(name, ".".concat(ext)) : name, fileName);
}
var migrationGroup = {
  name: "migration",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Manages content migrations for Content Lake datasets"
};
const maxKeyLength = function () {
  let children = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};
  let depth = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;
  return Object.entries(children).map(_ref7 => {
    let [key, child] = _ref7;
    return Math.max(key.length + depth * 2, maxKeyLength(child.children, depth + 1));
  }).reduce((max, next) => next > max ? next : max, 0);
};
const formatTree = _ref8 => {
  let {
    node = {},
    paddingLength,
    indent = "",
    getNodes: getLeaves = _ref9 => {
      let {
        nodes
      } = _ref9;
      return nodes;
    },
    getMessage
  } = _ref8;
  const entries = Object.entries(node);
  return entries.map((_ref10, index) => {
    let [key, child] = _ref10;
    const isLast = index === entries.length - 1;
    const nextIndent = "".concat(indent).concat(isLast ? "  " : "\u2502 ");
    const leaves = getLeaves(child);
    const nested = formatTree({
      node: child.children,
      paddingLength,
      indent: nextIndent,
      getNodes: getLeaves,
      getMessage
    });
    if (!(leaves == null ? void 0 : leaves.length)) {
      const current2 = "".concat(indent).concat(isLast ? "\u2514" : "\u251C", "\u2500 ").concat(key);
      return [current2, nested].filter(Boolean).join("\n");
    }
    const [first, ...rest] = leaves;
    const firstPadding = ".".repeat(paddingLength - indent.length - key.length);
    const elbow = isLast ? "\u2514" : "\u251C";
    const subsequentPadding = " ".repeat(paddingLength - indent.length + 2);
    const firstMessage = "".concat(indent).concat(elbow, "\u2500 ").concat(key, " ").concat(firstPadding, " ").concat(getMessage(first));
    const subsequentMessages = rest.map(marker => "".concat(nextIndent).concat(subsequentPadding, " ").concat(getMessage(marker))).join("\n");
    const current = [firstMessage, subsequentMessages].filter(Boolean).join("\n");
    return [current, nested].filter(Boolean).join("\n");
  }).join("\n");
};
function convertToTree(nodes) {
  const root = {};
  function addNode(node) {
    let tree = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : root;
    if (!node.path.length) {
      if (!tree.nodes) tree.nodes = [];
      tree.nodes.push(node);
      return;
    }
    const [current, ...rest] = node.path;
    const key = sanity.pathToString([current]);
    if (!tree.children) tree.children = {};
    if (!(key in tree.children)) tree.children[key] = {};
    addNode({
      ...node,
      path: rest
    }, tree.children[key]);
  }
  for (const node of nodes) addNode(node);
  return root;
}
const isTty = node_tty.isatty(1);
function prettyFormat(_ref11) {
  let {
    chalk,
    subject,
    migration,
    indentSize = 0
  } = _ref11;
  return (Array.isArray(subject) ? subject : [subject]).map(subjectEntry => {
    if (subjectEntry.type === "transaction") {
      return [[badge("transaction", "info", chalk), typeof subjectEntry.id === "undefined" ? null : chalk.underline(subjectEntry.id)].filter(Boolean).join(" "), indent(prettyFormat({
        chalk,
        subject: subjectEntry.mutations,
        migration,
        indentSize
      }))].join("\n\n");
    }
    return prettyFormatMutation({
      chalk,
      subject: subjectEntry,
      migration,
      indentSize
    });
  }).join("\n\n");
}
function encodeItemRef(ref) {
  return typeof ref === "number" ? ref : ref._key;
}
function badgeStyle(chalk, variant) {
  const styles = {
    info: chalk.bgWhite.black,
    incremental: chalk.bgGreen.black.bold,
    maybeDestructive: chalk.bgYellow.black.bold,
    destructive: chalk.bgRed.black.bold
  };
  return styles[variant];
}
function badge(label, variant, chalk) {
  if (!isTty) {
    return "[".concat(label, "]");
  }
  return badgeStyle(chalk, variant)(" ".concat(label, " "));
}
const mutationImpact = {
  create: "incremental",
  createIfNotExists: "incremental",
  createOrReplace: "maybeDestructive",
  delete: "destructive",
  patch: "maybeDestructive"
};
function documentId(mutation) {
  if ("id" in mutation) {
    return mutation.id;
  }
  if ("document" in mutation) {
    return mutation.document._id;
  }
  return void 0;
}
const listFormatter = new Intl.ListFormat("en-US", {
  type: "disjunction"
});
function mutationHeader(chalk, mutation, migration) {
  var _a;
  const mutationType = badge(mutation.type, mutationImpact[mutation.type], chalk);
  const documentType = "document" in mutation || migration.documentTypes ? badge("document" in mutation ? mutation.document._type : listFormatter.format((_a = migration.documentTypes) != null ? _a : []), "info", chalk) : null;
  return [mutationType, documentType, chalk.underline(documentId(mutation))].filter(Boolean).join(" ");
}
function prettyFormatMutation(_ref12) {
  let {
    chalk,
    subject,
    migration,
    indentSize = 0
  } = _ref12;
  var _a;
  const lock = "options" in subject ? chalk.cyan("(if revision==".concat((_a = subject.options) == null ? void 0 : _a.ifRevision, ")")) : "";
  const header = [mutationHeader(chalk, subject, migration), lock].join(" ");
  const padding = " ".repeat(indentSize);
  if (subject.type === "create" || subject.type === "createIfNotExists" || subject.type === "createOrReplace") {
    return [header, "\n", indent(JSON.stringify(subject.document, null, 2), indentSize)].join("");
  }
  if (subject.type === "patch") {
    const tree = convertToTree(subject.patches.flat());
    const paddingLength = Math.max(maxKeyLength(tree.children) + 2, 30);
    return [header, "\n", formatTree({
      node: tree.children,
      paddingLength,
      indent: padding,
      getMessage: patch => formatPatchMutation(chalk, patch)
    })].join("");
  }
  return header;
}
function formatPatchMutation(chalk, patch) {
  const {
    op
  } = patch;
  const formattedType = chalk.bold(op.type);
  if (op.type === "unset") {
    return "".concat(chalk.red(formattedType), "()");
  }
  if (op.type === "diffMatchPatch") {
    return "".concat(chalk.yellow(formattedType), "(").concat(op.value, ")");
  }
  if (op.type === "inc" || op.type === "dec") {
    return "".concat(chalk.yellow(formattedType), "(").concat(op.amount, ")");
  }
  if (op.type === "set") {
    return "".concat(chalk.yellow(formattedType), "(").concat(JSON.stringify(op.value), ")");
  }
  if (op.type === "setIfMissing") {
    return "".concat(chalk.green(formattedType), "(").concat(JSON.stringify(op.value), ")");
  }
  if (op.type === "insert") {
    return "".concat(chalk.green(formattedType), "(").concat(op.position, ", ").concat(encodeItemRef(op.referenceItem), ", ").concat(JSON.stringify(op.items), ")");
  }
  if (op.type === "replace") {
    return "".concat(chalk.yellow(formattedType), "(").concat(encodeItemRef(op.referenceItem), ", ").concat(JSON.stringify(op.items), ")");
  }
  if (op.type === "truncate") {
    return "".concat(chalk.red(formattedType), "(").concat(op.startIndex, ", ").concat(op.endIndex, ")");
  }
  throw new Error("Invalid operation type: ".concat(op.type));
}
function indent(subject) {
  let size = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 2;
  const padding = " ".repeat(size);
  return subject.split("\n").map(line => padding + line).join("\n");
}
const helpText$5 = "\nOptions\n  --no-dry-run By default the migration runs in dry mode. Pass this option to migrate dataset.\n  --concurrency <concurrent> How many mutation requests to run in parallel. Must be between 1 and ".concat(migrate.MAX_MUTATION_CONCURRENCY, ". Default: ").concat(migrate.DEFAULT_MUTATION_CONCURRENCY, ".\n  --no-progress Don't output progress. Useful if you want debug your migration script and see the output of console.log() statements.\n  --dataset <dataset> Dataset to migrate. Defaults to the dataset configured in your Sanity CLI config.\n  --project <project id> Project ID of the dataset to migrate. Defaults to the projectId configured in your Sanity CLI config.\n  --no-confirm Skip the confirmation prompt before running the migration. Make sure you know what you're doing before using this flag.\n  --from-export <export.tar.gz> Use a local dataset export as source for migration instead of calling the Sanity API. Note: this is only supported for dry runs.\n\n\nExamples\n  # dry run the migration\n  sanity migration run <id>\n\n  # execute the migration against a dataset\n  sanity migration run <id> --no-dry-run --project xyz --dataset staging\n\n  # execute the migration using a dataset export as the source\n  sanity migration run <id>  --from-export=production.tar.gz --no-dry-run --projectId xyz --dataset staging\n");
function parseCliFlags(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).options("dry-run", {
    type: "boolean",
    default: true
  }).options("concurrency", {
    type: "number",
    default: migrate.DEFAULT_MUTATION_CONCURRENCY
  }).options("progress", {
    type: "boolean",
    default: true
  }).options("dataset", {
    type: "string"
  }).options("from-export", {
    type: "string"
  }).options("project", {
    type: "string"
  }).options("confirm", {
    type: "boolean",
    default: true
  }).argv;
}
const runMigrationCommand = {
  name: "run",
  group: "migration",
  signature: "ID",
  helpText: helpText$5,
  description: "Run a migration against a dataset",
  // eslint-disable-next-line max-statements
  action: async (args, context) => {
    const {
      apiClient,
      output,
      prompt,
      chalk,
      workDir
    } = context;
    const [id] = args.argsWithoutOptions;
    const flags = await parseCliFlags(args);
    const fromExport = flags.fromExport;
    const dry = flags.dryRun;
    const dataset = flags.dataset;
    const project = flags.project;
    if (dataset && !project || project && !dataset) {
      throw new Error("If either --dataset or --project is provided, both must be provided");
    }
    if (!id) {
      output.error(chalk.red("Error: Migration ID must be provided"));
      const migrations = await resolveMigrations(workDir);
      const table = new consoleTablePrinter.Table({
        title: "Migrations found in project",
        columns: [{
          name: "id",
          title: "ID",
          alignment: "left"
        }, {
          name: "title",
          title: "Title",
          alignment: "left"
        }]
      });
      migrations.forEach(definedMigration => {
        table.addRow({
          id: definedMigration.id,
          title: definedMigration.migration.title
        });
      });
      table.printTable();
      output.print("\nRun `sanity migration run <ID>` to run a migration");
      return;
    }
    {
      node.register({
        target: "node".concat(process.version.slice(1))
      });
    }
    const candidates = resolveMigrationScript(workDir, id);
    const resolvedScripts = candidates.filter(isLoadableMigrationScript);
    if (resolvedScripts.length > 1) {
      throw new Error('Found multiple migrations for "'.concat(id, '" in current directory ').concat(candidates.map(candidate => candidate.relativePath).join(", ")));
    }
    const script = resolvedScripts[0];
    if (!script) {
      throw new Error('No migration found for "'.concat(id, '" in current directory. Make sure that the migration file exists and exports a valid migration as its default export.\n\n Tried the following files:\n - ').concat(candidates.map(candidate => candidate.relativePath).join("\n - ")));
    }
    const mod = script.mod;
    if ("up" in mod || "down" in mod) {
      throw new Error('Only "up" migrations are supported at this time, please use a default export');
    }
    const migration = mod.default;
    if (fromExport && !dry) {
      throw new Error("Can only dry run migrations from a dataset export file");
    }
    const concurrency = flags.concurrency;
    if (concurrency !== void 0) {
      if (concurrency > migrate.MAX_MUTATION_CONCURRENCY) {
        throw new Error("Concurrency exceeds the maximum allowed value of ".concat(migrate.MAX_MUTATION_CONCURRENCY));
      }
      if (concurrency === 0) {
        throw new Error("Concurrency must be a positive number, got ".concat(concurrency));
      }
    }
    const projectConfig = apiClient({
      requireUser: true,
      requireProject: true
    }).config();
    const apiConfig = {
      dataset: dataset != null ? dataset : projectConfig.dataset,
      projectId: project != null ? project : projectConfig.projectId,
      apiHost: projectConfig.apiHost,
      token: projectConfig.token,
      apiVersion: "v2024-01-29"
    };
    if (dry) {
      dryRunHandler();
      return;
    }
    const response = flags.confirm && (await prompt.single({
      message: "This migration will run on the ".concat(chalk.yellow(chalk.bold(apiConfig.dataset)), " dataset in ").concat(chalk.yellow(chalk.bold(apiConfig.projectId)), " project. Are you sure?"),
      type: "confirm"
    }));
    if (response === false) {
      debug$2("User aborted migration");
      return;
    }
    const spinner = output.spinner('Running migration "'.concat(id, '"')).start();
    await migrate.run({
      api: apiConfig,
      concurrency,
      onProgress: createProgress(spinner)
    }, migration);
    spinner.stop();
    function createProgress(progressSpinner) {
      return function onProgress(progress) {
        if (!flags.progress) {
          progressSpinner.stop();
          return;
        }
        if (progress.done) {
          progressSpinner.text = 'Migration "'.concat(id, '" completed.\n\n  Project id:  ').concat(chalk.bold(apiConfig.projectId), "\n  Dataset:     ").concat(chalk.bold(apiConfig.dataset), "\n\n  ").concat(progress.documents, " documents processed.\n  ").concat(progress.mutations, " mutations generated.\n  ").concat(chalk.green(progress.completedTransactions.length), " transactions committed.");
          progressSpinner.stopAndPersist({
            symbol: chalk.green("\u2714")
          });
          return;
        }
        [null, ...progress.currentTransactions].forEach(transaction => {
          var _a;
          progressSpinner.text = 'Running migration "'.concat(id, '" ').concat(dry ? "in dry mode..." : "...", "\n\n  Project id:     ").concat(chalk.bold(apiConfig.projectId), "\n  Dataset:        ").concat(chalk.bold(apiConfig.dataset), "\n  Document type:  ").concat(chalk.bold((_a = migration.documentTypes) == null ? void 0 : _a.join(",")), "\n\n  ").concat(progress.documents, " documents processed\u2026\n  ").concat(progress.mutations, " mutations generated\u2026\n  ").concat(chalk.blue(progress.pending), " requests pending\u2026\n  ").concat(chalk.green(progress.completedTransactions.length), " transactions committed.\n\n  ").concat(transaction && !progress.done ? "\xBB ".concat(prettyFormat({
            chalk,
            subject: transaction,
            migration,
            indentSize: 2
          })) : "");
        });
      };
    }
    async function dryRunHandler() {
      output.print('Running migration "'.concat(id, '" in dry mode'));
      if (fromExport) {
        output.print("Using export ".concat(chalk.cyan(fromExport)));
      }
      output.print();
      output.print("Project id:  ".concat(chalk.bold(apiConfig.projectId)));
      output.print("Dataset:     ".concat(chalk.bold(apiConfig.dataset)));
      for await (const mutation of migrate.dryRun({
        api: apiConfig,
        exportPath: fromExport
      }, migration)) {
        if (!mutation) continue;
        output.print();
        output.print(prettyFormat({
          chalk,
          subject: mutation,
          migration
        }));
      }
    }
  }
};
const helpText$4 = '\nNotes\n  Changing the hostname or port number might require a new entry to the CORS-origins allow list.\n\nOptions\n  --port <port> TCP port to start server on. [default: 3333]\n  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]\n\nExamples\n  sanity preview --host=0.0.0.0\n  sanity preview --port=1942\n  sanity preview some/build-output-dir\n';
const previewCommand = {
  name: "preview",
  signature: "[BUILD_OUTPUT_DIR] [--port <port>] [--host <host>]",
  description: "Starts a server to preview a production build of Sanity Studio",
  action: async (args, context) => {
    const previewAction = await getPreviewAction$1();
    return previewAction(args, context);
  },
  helpText: helpText$4
};
async function getPreviewAction$1() {
  const mod = await Promise.resolve().then(function () {
    return require('./previewAction-DgzrEWwB.js');
  });
  return mod.default;
}
var schemaGroup = {
  name: "schema",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Interacts with Sanity Studio schema configurations"
};
const description = "Validates all schema types specified in a workspace.";
const helpText$3 = "\nOptions\n  --workspace <name> The name of the workspace to use when validating all schema types.\n  --format <pretty|ndjson|json> The output format used to print schema errors and warnings.\n  --level <error|warning> The minimum level reported out. Defaults to warning.\n\nExamples\n  # Validates all schema types in a Sanity project with more than one workspace\n  sanity schema validate --workspace default\n\n  # Save the results of the report into a file\n  sanity schema validate > report.txt\n\n  # Report out only errors\n  sanity schema validate --level error\n";
const validateDocumentsCommand = {
  name: "validate",
  group: "schema",
  signature: "",
  description,
  helpText: helpText$3,
  action: async (args, context) => {
    const mod = await Promise.resolve().then(function () {
      return require('./validateAction-Cx6xAS3r.js');
    });
    return mod.default(args, context);
  }
};
const isInteractive = process.stdout.isTTY && process.env.TERM !== "dumb" && !("CI" in process.env);
const helpText$2 = '\nNotes\n  Changing the hostname or port number might require a new CORS-entry to be added.\n\nOptions\n  --port <port> TCP port to start server on. [default: 3333]\n  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]\n\nExamples\n  sanity start --host=0.0.0.0\n  sanity start --port=1942\n  sanity start some/build-output-dir\n';
const startCommand = {
  name: "start",
  signature: "[BUILD_OUTPUT_DIR] [--port <port>] [--host <host>]",
  description: "Alias for `sanity preview`",
  action: async (args, context) => {
    const {
      output,
      chalk,
      prompt
    } = context;
    const previewAction = await getPreviewAction();
    const warn = msg => output.warn(chalk.yellow.bgBlack(msg));
    const error = msg => output.warn(chalk.red.bgBlack(msg));
    warn("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E");
    warn("\u2502                                                           \u2502");
    warn("\u2502  You're running Sanity Studio v3. In this version the     \u2502");
    warn("\u2502  [start] command is used to preview static builds.        |");
    warn("\u2502                                                           \u2502");
    warn("\u2502  To run a development server, use the [npm run dev] or    |");
    warn("\u2502  [npx sanity dev] command instead. For more information,  \u2502");
    warn("\u2502  see https://www.sanity.io/help/studio-v2-vs-v3           \u2502");
    warn("\u2502                                                           \u2502");
    warn("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F");
    warn("");
    try {
      await previewAction(args, context);
    } catch (err) {
      if (err.name !== "BUILD_NOT_FOUND") {
        throw err;
      }
      error(err.message);
      error("\n");
      const shouldRunDevServer = isInteractive && (await prompt.single({
        message: "Do you want to start a development server instead?",
        type: "confirm"
      }));
      if (shouldRunDevServer) {
        const devAction = await getDevAction();
        await devAction(args, context);
      } else {
        process.exit(1);
      }
    }
  },
  helpText: helpText$2
};
async function getPreviewAction() {
  const mod = await Promise.resolve().then(function () {
    return require('./previewAction-DgzrEWwB.js');
  });
  return mod.default;
}
const uninstallCommand = {
  name: "uninstall",
  signature: "[plugin]",
  helpText: "",
  description: "Removes a Sanity plugin from the current Sanity configuration",
  hideFromHelp: true,
  action: async (args, context) => {
    await context.output.error("`sanity uninstall` is no longer supported - use npm/yarn");
  }
};
function prettifyQuotaError(message) {
  return err => {
    if (err.statusCode === 402) {
      err.message = message;
      throw err;
    }
    throw err;
  };
}
const helpText$1 = '\nOptions\n  --role Role to invite the user as\n\nExamples\n  # Invite a new user to the project (prompt for details)\n  sanity users invite\n\n  # Send a new user invite to the email "pippi@sanity.io", prompt for role\n  sanity users invite pippi@sanity.io\n\n  # Send a new user invite to the email "pippi@sanity.io", as administrator\n  sanity users invite pippi@sanity.io --role administrator\n';
const inviteUserCommand = {
  name: "invite",
  group: "users",
  signature: "[EMAIL]",
  helpText: helpText$1,
  description: "Invite a new user to the project",
  action: async (args, context) => {
    const {
      apiClient,
      output,
      prompt
    } = context;
    const [selectedEmail] = args.argsWithoutOptions;
    const flags = args.extOptions;
    const client = apiClient().clone().config({
      useProjectHostname: false,
      apiVersion: "2021-06-07"
    });
    const {
      projectId
    } = client.config();
    const roles = (await client.request({
      uri: "/projects/".concat(projectId, "/roles")
    })).filter(role2 => role2.appliesToUsers);
    const email = selectedEmail || (await promptForEmail(prompt));
    const selectedRole = flags.role || (await promptForRole(prompt, roles));
    const role = roles.find(_ref13 => {
      let {
        name
      } = _ref13;
      return name.toLowerCase() === selectedRole.toLowerCase();
    });
    if (!role) {
      throw new Error('Role name "'.concat(selectedRole, '" not found'));
    }
    await client.clone().request({
      method: "POST",
      uri: "/invitations/project/".concat(projectId),
      body: {
        email,
        role: role.name
      },
      useGlobalApi: true,
      maxRedirects: 0
    }).catch(prettifyQuotaError("Project is already at user quota, add billing details to the project in order to allow overage charges."));
    output.print("Invitation sent to ".concat(email));
  }
};
function promptForEmail(prompt) {
  return prompt.single({
    type: "input",
    message: "Email to invite:",
    filter: val => val.trim(),
    validate: name => {
      if (!name || !name.includes("@")) {
        return "Invalid email";
      }
      return true;
    }
  });
}
function promptForRole(prompt, roles) {
  return prompt.single({
    type: "list",
    message: "Which role should the user have?",
    choices: roles.map(role => ({
      value: role.name,
      name: "".concat(role.title, " (").concat(role.description, ")")
    }))
  });
}
const sortFields = ["id", "name", "role", "date"];
const helpText = "\nOptions\n  --no-invitations Don't include pending invitations\n  --no-robots Don't include robots (token users)\n  --sort <field> Sort users by specified column: ".concat(sortFields.join(", "), "\n  --order <asc/desc> Sort output ascending/descending\n\nExamples\n  # List all users of the project\n  sanity users list\n\n  # List all users of the project, but exclude pending invitations and robots\n  sanity users list --no-invitations --no-robots\n\n  # List all users, sorted by role\n  sanity users list --sort role\n");
const listUsersCommand = {
  name: "list",
  group: "users",
  signature: "",
  helpText,
  description: "List all users of the project",
  action: async (args, context) => {
    const {
      apiClient,
      output,
      chalk
    } = context;
    const {
      sort,
      order,
      robots,
      invitations
    } = {
      sort: "date",
      order: "asc",
      robots: true,
      invitations: true,
      ...args.extOptions
    };
    if (!sortFields.includes(sort)) {
      throw new Error("Can't sort by field \"".concat(sort, '". Must be one of ').concat(sortFields.join(", ")));
    }
    if (order !== "asc" && order !== "desc") {
      throw new Error('Unknown sort order "'.concat(order, '", must be either "asc" or "desc"'));
    }
    const client = apiClient();
    const globalClient = client.clone().config({
      useProjectHostname: false
    });
    const {
      projectId
    } = client.config();
    const useGlobalApi = true;
    const [pendingInvitations, project] = await Promise.all([invitations ? globalClient.request({
      uri: "/invitations/project/".concat(projectId),
      useGlobalApi
    }).then(getPendingInvitations) : [], globalClient.request({
      uri: "/projects/".concat(projectId),
      useGlobalApi
    })]);
    const memberIds = project.members.map(member => member.id);
    const users = await globalClient.request({
      uri: "/users/".concat(memberIds.join(",")),
      useGlobalApi
    }).then(user => Array.isArray(user) ? user : [user]);
    const projectMembers = project.members.map(member => ({
      ...member,
      ...getUserProps(users.find(candidate => candidate.id === member.id))
    })).filter(member => !member.isRobot || robots);
    const members = [...projectMembers, ...pendingInvitations];
    const ordered = sortBy__default.default(members.map(_ref14 => {
      let {
        id,
        name,
        role,
        date
      } = _ref14;
      return [id, name, role, date];
    }), [sortFields.indexOf(sort)]);
    const rows = order === "asc" ? ordered : ordered.reverse();
    const maxWidths = rows.reduce((max, row) => row.map((current, index) => Math.max(size__default.default(current), max[index])), sortFields.map(str => size__default.default(str)));
    const printRow = row => {
      const isInvite = row[0] === "<pending>";
      const textRow = row.map((col, i) => "".concat(col).padEnd(maxWidths[i])).join("   ");
      return isInvite ? chalk.dim(textRow) : textRow;
    };
    output.print(chalk.cyan(printRow(sortFields)));
    rows.forEach(row => output.print(printRow(row)));
  }
};
function getUserProps(user) {
  const {
    displayName: name,
    createdAt: date
  } = user || {};
  return {
    name: name || "",
    date: date || ""
  };
}
function getPendingInvitations(invitations) {
  return invitations.filter(invite => !invite.isAccepted && !invite.isRevoked && !invite.acceptedByUserId).map(invite => ({
    id: "<pending>",
    name: invite.email,
    role: invite.role,
    date: invite.createdAt
  }));
}
const usersGroup = {
  name: "users",
  signature: "[COMMAND]",
  isGroupRoot: true,
  description: "Manages users of your Sanity project"
};
const commands = [buildCommand, checkCommand, configCheckCommand, datasetGroup, deployCommand, undeployCommand, listDatasetsCommand, createDatasetCommand, datasetVisibilityCommand, exportDatasetCommand, importDatasetCommand, deleteDatasetCommand, copyDatasetCommand, aliasCommand, datasetBackupGroup, listDatasetBackupCommand, downloadBackupCommand, disableDatasetBackupCommand, enableDatasetBackupCommand, corsGroup, listCorsOriginsCommand, addCorsOriginCommand, deleteCorsOriginCommand, usersGroup, inviteUserCommand, listUsersCommand, hookGroup, listHooksCommand, createHookCommand, migrationGroup, createMigrationCommand, runMigrationCommand, listMigrationCommand, deleteHookCommand, listHookLogsCommand, printHookAttemptCommand, documentsGroup, getDocumentsCommand, queryDocumentsCommand, deleteDocumentsCommand, createDocumentsCommand, validateDocumentsCommand$1, graphqlGroup, listGraphQLAPIsCommand, deployGraphQLAPICommand, deleteGraphQLAPICommand, devCommand, startCommand, schemaGroup, validateDocumentsCommand, previewCommand, uninstallCommand, execCommand];
const cliProjectCommands = {
  requiredCliVersionRange: "^3.0.0",
  commands
};
exports.cliProjectCommands = cliProjectCommands;
exports.convertToTree = convertToTree;
exports.debug = debug$2;
exports.formatTree = formatTree;
exports.getClientUrl = getClientUrl;
exports.maxKeyLength = maxKeyLength;
//# sourceMappingURL=_internal-hHVP4WHZ.js.map
